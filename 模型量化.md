
# Definition

模型量化是一种优化技术，用于减少机器学习模型的大小和提高推理速度，同时尽量减少模型精度的损失。它通过将模型中的浮点数参数（通常是32位浮点数）转换为较低精度的表示形式（如8位整数或16位浮点数）来实现这一目标。

模型量化的主要优点包括：

1. **减少模型大小**：通过减少每个参数的位数，可以显著减少模型的存储空间需求。
    
2. **提高推理速度**：较低精度的计算通常可以在硬件上更快地执行，尤其是在支持量化运算的专用硬件（如某些AI加速器）上。
    
3. **降低能耗**：较少的位数意味着更少的计算量和内存访问，从而降低能耗。

本质上讲，模型量化就是在干一件事情：==***转换数据类型（大数据类型映射到小数据类型）***==，是一种有损压缩的方法。

# Frequently Used Data Type

![[Frequently Used Data Type in DL.png]]

FP32是最为常见的单精度浮点数类型，也是平时模型训练的时候应用最为广泛的数据类型。其中特点就是精度很高并且表示的数据范围很大，缺点也很明显：占用的空间非常大！为此FP16诞生了，属于半精度浮点数类型，但是FP16表达的最大数字也就是64K，所以FP16很容出现上溢和下溢的情况，在神经网络训练中表示就是"NaN"（Not a Number）。同样的，为了解决这个问题，BF16出现了，BF16将FP16的指数位扩大了很多，牺牲了一定的小数位，也就是用一定的精度来换取更大的数据范围，从而保证了不会出现数据上溢和下溢。而TF32则是更加有意思，将FP16的精度和BF16的大范围的优点结合到了一起，使用19位来表示。（Actually，这个只能在Ampere架构及之后的GPU使用，而且TF32是一种计算数据类型而不是存储数据类型，所以操作范围有限）

一个关于量化的重要事实就是（基于大量的实践）：==在推理的时候，模型的的半精度weight往往能够提供单精度一样的效果，而只有在模型进行梯度更新的时候才完全需要单精度。==这意味着在模型训练中往往主导的weight是FP32，但是推理使用FP16，是一种混合精度的方法。

# Two Basic Quantization Approach

## 零点量化方法（Zero-Point Quantization）

这种量化方法首先确定“0”的新位置，然后在使用基于“0”的偏移来确定每一个数字的位置。是一种对称的量化方法。

![[零点量化方法.png]]

假设我们有一个浮点数张量 $F$，其值范围在 $[F_{\text{min}}, F_{\text{max}}]$ 之间。我们希望将其量化为 $b$ 位整数，整数范围为 $[Q_{\text{min}}, Q_{\text{max}}]$。

1. **缩放因子（scale）**：
   $$S = \frac{F_{\text{max}} - F_{\text{min}}}{Q_{\text{max}} - Q_{\text{min}}}$$

2. **零点（zero-point）**：
   $$
   Z = Q_{\text{min}} - \frac{F_{\text{min}}}{S}
   $$

3. **量化过程**：
   $$
   Q(x) = \text{round}(x / S + Z)
   $$

4. **反量化过程**：
   $$
   F(q) = (q - Z) \times S
   $$

## 最大绝对值量化（Max-Abs Quantization）

最大绝对值量化是一种非对称量化方法，它直接使用浮点数的最大绝对值来确定量化范围。这种方法通常用于将浮点数映射到有符号整数范围。

![[最大绝对值量化方法.png]]

假设我们有一个浮点数张量 $F$，其值范围在 $[-F_{\text{abs\_max}}, F_{\text{abs\_max}}]$ 之间。我们希望将其量化为 $b$ 位有符号整数，整数范围为 $[-Q_{\text{abs\_max}}, Q_{\text{abs\_max}}]$。

1. **缩放因子（scale）**：
   $$
   S = \frac{F_{\text{abs\_max}}}{Q_{\text{abs\_max}}}
   $$

2. **量化过程**：
   $$
   Q(x) = \text{round}(x / S)
   $$

3. **反量化过程**：
   $$
   F(q) = q \times S
   $$
## 符号说明

- $F$：浮点数张量
- $F_{\text{min}}$：浮点数张量的最小值
- $F_{\text{max}}$：浮点数张量的最大值
- $F_{\text{abs\_max}}$：浮点数张量的最大绝对值
- $Q$：量化后的整数张量
- $Q_{\text{min}}$：量化整数的最小值
- $Q_{\text{max}}$：量化整数的最大值
- $Q_{\text{abs\_max}}$：量化整数的最大绝对值
- $S$：缩放因子
- $Z$：零点
- $b$：量化位数

## 量化方法的组合

>当进行矩阵乘法时，可以通过组合各种技巧，例如逐行或逐向量量化，来获取更精确的结果。举个例子，对矩阵乘法，我们不会直接使用常规量化方式，即用整个张量的最大绝对值对张量进行归一化，而会转而使用向量量化方法，找到 A 的每一行和 B 的每一列的最大绝对值，然后逐行或逐列归一化 A 和 B 。最后将 A 与 B 相乘得到 C。最后，我们再计算与 A 和 B 的最大绝对值向量的外积，并将此与 C 求哈达玛积来反量化回 FP16。

由于这个表达很晦涩，所以这个方法我使用了GPT4o生成了一个很好的例子来说明。

这段话描述了一种用于矩阵乘法的量化方法，该方法结合了逐行或逐向量量化技巧，以提高结果的精度。具体步骤如下：

1. **逐行或逐列量化**：
   - 对于矩阵 $A$ 的每一行和矩阵 $B$ 的每一列，找到它们的最大绝对值。
   - 分别用这些最大值对对应的行和列进行归一化（量化）。

2. **进行矩阵乘法**：
   - 用量化后的矩阵 $A$ 和 $B$ 进行矩阵乘法，得到量化后的结果矩阵 $C$。

3. **反量化**：
   - 计算矩阵 $A$ 和 $B$ 的最大绝对值向量的外积，得到一个用于反量化的矩阵。
   - 将这个反量化矩阵与量化后的结果矩阵 $C$ 进行哈达玛积（逐元素相乘），以将结果反量化回浮点数形式（如FP16）。

### 具体步骤和示例

假设我们有两个矩阵 $A$ 和 $B$：

$$ A = \begin{bmatrix} 0.1 & -0.5 \\ 1.2 & -3.0 \\ \end{bmatrix}, \quad B = \begin{bmatrix} 2.5 & 1.0 \\ -1.0 & 0.5 \\ \end{bmatrix} $$

#### 步骤 1: 逐行或逐列量化

1. **找到每行和每列的最大绝对值**：
   - $A$ 的行最大值：$$ [0.5, 3.0] $$
   - $B$ 的列最大值：$$ [2.5, 1.0] $$

2. **逐行量化 $A$ 和逐列量化 $B$**：
   - 量化 $A$：
     $$ A_{quant} = \begin{bmatrix} 0.1/0.5 & -0.5/0.5 \\ 1.2/3.0 & -3.0/3.0 \\ \end{bmatrix} = \begin{bmatrix} 0.2 & -1.0 \\ 0.4 & -1.0 \\ \end{bmatrix} $$
   - 量化 $B$：
     $$ B_{quant} = \begin{bmatrix} 2.5/2.5 & 1.0/1.0 \\ -1.0/2.5 & 0.5/1.0 \\ \end{bmatrix} = \begin{bmatrix} 1.0 & 1.0 \\ -0.4 & 0.5 \\ \end{bmatrix} $$

#### 步骤 2: 进行矩阵乘法

计算量化后的矩阵乘法 $C_{quant}$：
$$ C_{quant} = A_{quant} \times B_{quant} = \begin{bmatrix} 0.2 & -1.0 \\ 0.4 & -1.0 \\ \end{bmatrix} \times \begin{bmatrix} 1.0 & 1.0 \\ -0.4 & 0.5 \\ \end{bmatrix} = \begin{bmatrix} 0.2 \times 1.0 + (-1.0) \times (-0.4) & 0.2 \times 1.0 + (-1.0) \times 0.5 \\ 0.4 \times 1.0 + (-1.0) \times (-0.4) & 0.4 \times 1.0 + (-1.0) \times 0.5 \\ \end{bmatrix} = \begin{bmatrix} 0.6 & -0.3 \\ 0.8 & -0.1 \\ \end{bmatrix} $$

#### 步骤 3: 反量化

1. **计算最大绝对值向量的外积**：
   $$ \text{max\_A} = [0.5, 3.0], \quad \text{max\_B} = [2.5, 1.0] $$
   $$ \text{max\_A} \otimes \text{max\_B} = \begin{bmatrix} 0.5 \times 2.5 & 0.5 \times 1.0 \\ 3.0 \times 2.5 & 3.0 \times 1.0 \\ \end{bmatrix} = \begin{bmatrix} 1.25 & 0.5 \\ 7.5 & 3.0 \\ \end{bmatrix} $$

2. **计算反量化结果**：
   $$ C_{final} = C_{quant} \circ (\text{max\_A} \otimes \text{max\_B}) = \begin{bmatrix} 0.6 & -0.3 \\ 0.8 & -0.1 \\ \end{bmatrix} \circ \begin{bmatrix} 1.25 & 0.5 \\ 7.5 & 3.0 \\ \end{bmatrix} = \begin{bmatrix} 0.6 \times 1.25 & -0.3 \times 0.5 \\ 0.8 \times 7.5 & -0.1 \times 3.0 \\ \end{bmatrix} = \begin{bmatrix} 0.75 & -0.15 \\ 6.0 & -0.3 \\ \end{bmatrix} $$

最终结果 $C$ 被反量化回浮点数形式。




# LLM.int8()

上面的基本量化方法虽然初步的造成的误差不高，但是随着模型的大小增长，误差会开始累积并且逐渐影响模型的性能。Hugging Face Transformers 和 Accelerate 库中的 LLM.int8() 是第一个适用于大模型 (如 BLOOM-176B) 且不会降低准确性的量化技术。


















# 参考文献

1. [LLM（十一）：大语言模型的模型量化(INT8/INT4)技术](https://zhuanlan.zhihu.com/p/627436535)
2. [大规模 Transformer 模型 8 比特矩阵乘简介](https://huggingface.co/blog/zh/hf-bitsandbytes-integration)
3. [LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale](https://arxiv.org/abs/2208.07339)
	1. [中文版本翻译](https://yiyibooks.cn/arxiv/2208.07339v2/index.html)
4. 