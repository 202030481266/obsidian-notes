
# Definition

模型量化是一种优化技术，用于减少机器学习模型的大小和提高推理速度，同时尽量减少模型精度的损失。它通过将模型中的浮点数参数（通常是32位浮点数）转换为较低精度的表示形式（如8位整数或16位浮点数）来实现这一目标。

模型量化的主要优点包括：

1. **减少模型大小**：通过减少每个参数的位数，可以显著减少模型的存储空间需求。
    
2. **提高推理速度**：较低精度的计算通常可以在硬件上更快地执行，尤其是在支持量化运算的专用硬件（如某些AI加速器）上。
    
3. **降低能耗**：较少的位数意味着更少的计算量和内存访问，从而降低能耗。

本质上讲，模型量化就是在干一件事情：==***转换数据类型（大数据类型映射到小数据类型）***==，是一种有损压缩的方法。

# Frequently Used Data Type

![[Frequently Used Data Type in DL.png]]

FP32是最为常见的单精度浮点数类型，也是平时模型训练的时候应用最为广泛的数据类型。其中特点就是精度很高并且表示的数据范围很大，缺点也很明显：占用的空间非常大！为此FP16诞生了，属于半精度浮点数类型，但是FP16表达的最大数字也就是64K，所以FP16很容出现上溢和下溢的情况，在神经网络训练中表示就是"NaN"（Not a Number）。同样的，为了解决这个问题，BF16出现了，BF16将FP16的指数位扩大了很多，牺牲了一定的小数位，也就是用一定的精度来换取更大的数据范围，从而保证了不会出现数据上溢和下溢。而TF32则是更加有意思，将FP16的精度和BF16的大范围的优点结合到了一起，使用19位来表示。（Actually，这个只能在Ampere架构及之后的GPU使用，而且TF32是一种计算数据类型而不是存储数据类型，所以操作范围有限）

一个关于量化的重要事实就是（基于大量的实践）：==在推理的时候，模型的的半精度weight往往能够提供单精度一样的效果，而只有在模型进行梯度更新的时候才完全需要单精度。==这意味着在模型训练中往往主导的weight是FP32，但是推理使用FP16，是一种混合精度的方法。

# Two Basic Quantization Approach

## 零点量化方法（Zero-Point Quantization）

这种量化方法首先确定“0”的新位置，然后在使用基于“0”的偏移来确定每一个数字的位置。是一种对称的量化方法。

![[零点量化方法.png]]

假设我们有一个浮点数张量 $F$，其值范围在 $[F_{\text{min}}, F_{\text{max}}]$ 之间。我们希望将其量化为 $b$ 位整数，整数范围为 $[Q_{\text{min}}, Q_{\text{max}}]$。

1. **缩放因子（scale）**：
   $$S = \frac{F_{\text{max}} - F_{\text{min}}}{Q_{\text{max}} - Q_{\text{min}}}$$

2. **零点（zero-point）**：
   $$
   Z = Q_{\text{min}} - \frac{F_{\text{min}}}{S}
   $$

3. **量化过程**：
   $$
   Q(x) = \text{round}(x / S + Z)
   $$

4. **反量化过程**：
   $$
   F(q) = (q - Z) \times S
   $$

## 最大绝对值量化（Max-Abs Quantization）

最大绝对值量化是一种非对称量化方法，它直接使用浮点数的最大绝对值来确定量化范围。这种方法通常用于将浮点数映射到有符号整数范围。

![[最大绝对值量化方法.png]]

假设我们有一个浮点数张量 $F$，其值范围在 $[-F_{\text{abs\_max}}, F_{\text{abs\_max}}]$ 之间。我们希望将其量化为 $b$ 位有符号整数，整数范围为 $[-Q_{\text{abs\_max}}, Q_{\text{abs\_max}}]$。

1. **缩放因子（scale）**：
   $$
   S = \frac{F_{\text{abs\_max}}}{Q_{\text{abs\_max}}}
   $$

2. **量化过程**：
   $$
   Q(x) = \text{round}(x / S)
   $$

3. **反量化过程**：
   $$
   F(q) = q \times S
   $$
## 符号说明

- $F$：浮点数张量
- $F_{\text{min}}$：浮点数张量的最小值
- $F_{\text{max}}$：浮点数张量的最大值
- $F_{\text{abs\_max}}$：浮点数张量的最大绝对值
- $Q$：量化后的整数张量
- $Q_{\text{min}}$：量化整数的最小值
- $Q_{\text{max}}$：量化整数的最大值
- $Q_{\text{abs\_max}}$：量化整数的最大绝对值
- $S$：缩放因子
- $Z$：零点
- $b$：量化位数

## 量化方法的组合

>当进行矩阵乘法时，可以通过组合各种技巧，例如逐行或逐向量量化，来获取更精确的结果。举个例子，对矩阵乘法，我们不会直接使用常规量化方式，即用整个张量的最大绝对值对张量进行归一化，而会转而使用向量量化方法，找到 A 的每一行和 B 的每一列的最大绝对值，然后逐行或逐列归一化 A 和 B 。最后将 A 与 B 相乘得到 C。最后，我们再计算与 A 和 B 的最大绝对值向量的外积，并将此与 C 求哈达玛积来反量化回 FP16。

由于这个表达很晦涩，所以这个方法我使用了GPT4o生成了一个很好的例子来说明。

这段话描述了一种用于矩阵乘法的量化方法，该方法结合了逐行或逐向量量化技巧，以提高结果的精度。具体步骤如下：

1. **逐行或逐列量化**：
   - 对于矩阵 $A$ 的每一行和矩阵 $B$ 的每一列，找到它们的最大绝对值。
   - 分别用这些最大值对对应的行和列进行归一化（量化）。

2. **进行矩阵乘法**：
   - 用量化后的矩阵 $A$ 和 $B$ 进行矩阵乘法，得到量化后的结果矩阵 $C$。

3. **反量化**：
   - 计算矩阵 $A$ 和 $B$ 的最大绝对值向量的外积，得到一个用于反量化的矩阵。
   - 将这个反量化矩阵与量化后的结果矩阵 $C$ 进行哈达玛积（逐元素相乘），以将结果反量化回浮点数形式（如FP16）。

### 具体步骤和示例

假设我们有两个矩阵 $A$ 和 $B$：

$$ A = \begin{bmatrix} 0.1 & -0.5 \\ 1.2 & -3.0 \\ \end{bmatrix}, \quad B = \begin{bmatrix} 2.5 & 1.0 \\ -1.0 & 0.5 \\ \end{bmatrix} $$

#### 步骤 1: 逐行或逐列量化

1. **找到每行和每列的最大绝对值**：
   - $A$ 的行最大值：$$ [0.5, 3.0] $$
   - $B$ 的列最大值：$$ [2.5, 1.0] $$

2. **逐行量化 $A$ 和逐列量化 $B$**：
   - 量化 $A$：
     $$ A_{quant} = \begin{bmatrix} 0.1/0.5 & -0.5/0.5 \\ 1.2/3.0 & -3.0/3.0 \\ \end{bmatrix} = \begin{bmatrix} 0.2 & -1.0 \\ 0.4 & -1.0 \\ \end{bmatrix} $$
   - 量化 $B$：
     $$ B_{quant} = \begin{bmatrix} 2.5/2.5 & 1.0/1.0 \\ -1.0/2.5 & 0.5/1.0 \\ \end{bmatrix} = \begin{bmatrix} 1.0 & 1.0 \\ -0.4 & 0.5 \\ \end{bmatrix} $$

#### 步骤 2: 进行矩阵乘法

计算量化后的矩阵乘法 $C_{quant}$：
$$ C_{quant} = A_{quant} \times B_{quant} = \begin{bmatrix} 0.2 & -1.0 \\ 0.4 & -1.0 \\ \end{bmatrix} \times \begin{bmatrix} 1.0 & 1.0 \\ -0.4 & 0.5 \\ \end{bmatrix} = \begin{bmatrix} 0.2 \times 1.0 + (-1.0) \times (-0.4) & 0.2 \times 1.0 + (-1.0) \times 0.5 \\ 0.4 \times 1.0 + (-1.0) \times (-0.4) & 0.4 \times 1.0 + (-1.0) \times 0.5 \\ \end{bmatrix} = \begin{bmatrix} 0.6 & -0.3 \\ 0.8 & -0.1 \\ \end{bmatrix} $$

#### 步骤 3: 反量化

1. **计算最大绝对值向量的外积**：
   $$ \text{max\_A} = [0.5, 3.0], \quad \text{max\_B} = [2.5, 1.0] $$
   $$ \text{max\_A} \otimes \text{max\_B} = \begin{bmatrix} 0.5 \times 2.5 & 0.5 \times 1.0 \\ 3.0 \times 2.5 & 3.0 \times 1.0 \\ \end{bmatrix} = \begin{bmatrix} 1.25 & 0.5 \\ 7.5 & 3.0 \\ \end{bmatrix} $$

2. **计算反量化结果**：
   $$ C_{final} = C_{quant} \circ (\text{max\_A} \otimes \text{max\_B}) = \begin{bmatrix} 0.6 & -0.3 \\ 0.8 & -0.1 \\ \end{bmatrix} \circ \begin{bmatrix} 1.25 & 0.5 \\ 7.5 & 3.0 \\ \end{bmatrix} = \begin{bmatrix} 0.6 \times 1.25 & -0.3 \times 0.5 \\ 0.8 \times 7.5 & -0.1 \times 3.0 \\ \end{bmatrix} = \begin{bmatrix} 0.75 & -0.15 \\ 6.0 & -0.3 \\ \end{bmatrix} $$

最终结果 $C$ 被反量化回浮点数形式。

# LLM.int8()

上面的基本量化方法虽然初步的造成的误差不高，但是随着模型的大小增长，误差会开始累积并且逐渐影响模型的性能。Hugging Face Transformers 和 Accelerate 库中的 LLM.int8() 是第一个适用于大模型 (如 BLOOM-176B) 且不会降低准确性的量化技术。

## LLM.int8()算法描述

LLM.int8() 算法本身如下。本质上，LLM.int8() 通过三个步骤完成矩阵乘法计算:

1. 从输入的隐含状态中，按列提取==异常值== (即大于某个阈值的值)。
2. 对 FP16 离群值矩阵和 Int8 非离群值矩阵分别作矩阵乘法。
3. 反量化非离群值的矩阵乘结果并其与离群值矩阵乘结果相加，获得最终的 FP16 结果。

![[LLM.int8() 矩阵乘法内部展示图.png]]

## 离群值

超出某个分布范围的值通常叫做离群值。

> 我们观察到对于参数量大于 6B 的 transformer 模型，经典的量化方法会失效。虽然离群值特征也存在于较小的模型中，但在大于 6B 的 transformer 模型中，我们观察到几乎每层都会出现超出特定阈值的离群点，而且这些离群点呈现出一定的系统性模式。

在LLM.int8()中，transformer模型表现出来的涌现特性会随着传统量化失效，**其中的核心原因就是离群值的分布**，如前所述，8 位精度的动态范围极其有限，因此量化具有多个大值的向量会产生严重误差。此外，由于 transformer 架构的固有特性，它会将所有元素互相关联起来，这样的话，这些误差在传播几层后往往会混杂在一起。（误差传播的特性）

## 0退化

下面是一些作者在使用模型量化后的与没有进行量化的推理性能的对比，可以发现其中的性能的绝对值差异都小于原有模型性能的标准误差，所以可以看到没有性能下降。

对 OPT-175B 模型:

|    测试基准    |    -     |     -      |     -      |     -      |   差值   |
| :--------: | :------: | :--------: | :--------: | :--------: | :----: |
|   测试基准名    |    指标    | 指标值 - int8 | 指标值 - fp16 | 标准差 - fp16 |   -    |
| hellaswag  | acc_norm |   0.7849   |   0.7849   |   0.0041   |   0    |
| hellaswag  |   acc    |   0.5921   |   0.5931   |   0.0049   | 0.001  |
|    piqa    |   acc    |   0.7965   |   0.7959   |   0.0094   | 0.0006 |
|    piqa    | acc_norm |   0.8101   |   0.8107   |   0.0091   | 0.0006 |
|  lambada   |   ppl    |   3.0142   |   3.0152   |   0.0552   | 0.001  |
|  lambada   |   acc    |   0.7464   |   0.7466   |   0.0061   | 0.0002 |
| winogrande |   acc    |   0.7174   |   0.7245   |   0.0125   | 0.0071 |

对 BLOOM-176 模型:

|    测试基准    |    -     |     -      |     -      |     -      |   差值   |
| :--------: | :------: | :--------: | :--------: | :--------: | :----: |
|   测试基准名    |    指标    | 指标值 - int8 | 指标值 - fp16 | 标准差 - fp16 |   -    |
| hellaswag  | acc_norm |   0.7274   |   0.7303   |   0.0044   | 0.0029 |
| hellaswag  |   acc    |   0.5563   |   0.5584   |   0.005    | 0.0021 |
|    piqa    |   acc    |   0.7835   |   0.7884   |   0.0095   | 0.0049 |
|    piqa    | acc_norm |   0.7922   |   0.7911   |   0.0095   | 0.0011 |
|  lambada   |   ppl    |   3.9191   |   3.931    |   0.0846   | 0.0119 |
|  lambada   |   acc    |   0.6808   |   0.6718   |   0.0065   | 0.009  |
| winogrande |   acc    |   0.7048   |   0.7048   |   0.0128   |   0    |

## LLM.int8()量化对于推理速度的影响

看起来使用了int8的类型不仅减少了显存的占用，而且整数的计算应该是比浮点数更加迅速的，整体来看应该加快了推理的速度。但事实是int8()整体上缓慢了很多，特别是对于参数量较小的模型，具体的原因参考下面的issue链接。

[issue#6 Memory Decreases! But Latency Increases](https://github.com/bitsandbytes-foundation/bitsandbytes/issues/6)
[qwen2的模型效率测试](https://qwen.readthedocs.io/zh-cn/latest/benchmark/speed_benchmark.html)

|  精度  | 参数量  |      硬件      | 每词元延迟 (单位: 毫秒，batch size: 1) | 每词元延迟 (单位: 毫秒，batch size: 8) | 每词元延迟 (单位: 毫秒，batch size: 32) |
| :--: | :--: | :----------: | :--------------------------: | :--------------------------: | :---------------------------: |
| bf16 | 176B | 8xA100 80GB  |             239              |              32              |              9.9              |
| int8 | 176B | 4xA100 80GB  |             282              |             37.5             |             10.2              |
| bf16 | 176B | 14xA100 40GB |             285              |             36.5             |             10.4              |
| int8 | 176B | 5xA100 40GB  |             367              |             46.4             |              oom              |
| fp16 | 11B  |  2xT4 15GB   |             11.7             |             1.7              |              0.5              |
| int8 | 11B  |  1xT4 15GB   |             43.5             |             5.3              |              1.3              |
| fp32 |  3B  |  2xT4 15GB   |              45              |             7.2              |              3.1              |
| int8 |  3B  |  1xT4 15GB   |             312              |             39.1             |             10.2              |

### 问题总结

1. **CUDA核函数效率低**：
   开发人员为了提高内存效率，快速写了一个CUDA核函数，用于从特殊格式的矩阵中提取异常值。但这个核函数效率不高。

2. **fp16和int8矩阵乘法在同一CUDA流中运行**：
   当前fp16（半精度浮点）矩阵乘法和int8（8位整数）矩阵乘法在同一个CUDA流中运行，==这使得处理变成了顺序的==，尽管这两种乘法本来是独立的，可以并行处理。

3. **fp16矩阵乘法核函数未完全优化**：
   fp16矩阵乘法核函数可能没有针对极端矩阵大小进行完全优化。定制的核函数会非常快，但需要一些开发工作。

4. **int8矩阵乘法在小模型上不快**：
   对于小模型来说，int8矩阵乘法并不快。因为很难用int8元素完全占用GPU核心，所以int8的速度与fp16差不多。另外，==量化过程的额外开销还会拖慢整体推理速度==。对于一个6B（60亿参数）模型，可能的原始速度提升只有20-40%。但推理速度还取决于很多因素（比如序列长度、批次大小等），所以不确定。

### 影响排序

开发人员没有做精确的基准测试，但他们猜测了哪些问题对系统速度的影响最大，并给每个问题分配了一个权重，总和为100%：

1. **CUDA核函数效率低**：10%
2. **fp16和int8矩阵乘法在同一CUDA流中运行**：20%
3. **fp16矩阵乘法核函数未完全优化**：60%
4. **硬件问题**：10%

### 解决方案总结

简而言之，最有效的改进措施是：

1. **优化fp16矩阵乘法核函数**：这是最重要的改进措施，可能带来最大的性能提升。
2. **让fp16矩阵乘法在第二个CUDA流中运行**：这可以让处理并行化，从而提高速度。
3. **改进CUDA核函数用于提取异常值**：这可以提高内存效率。
4. **硬件问题**：无法解决，但影响较小。

# 代码集成

接下来让我们讨论在 Hugging Face `transformers` 集成该方法的细节，向你展示常见的用法及在使用过程中可能遇到的常见问题。

## 用法

所有的操作都集成在 `Linear8bitLt` 模块中，你可以轻松地从 `bitsandbytes` 库中导入它。它是 `torch.nn.modules` 的子类，你可以仿照下述代码轻松地将其应用到自己的模型中。

下面以使用 `bitsandbytes` 将一个小模型转换为 int8 为例，并给出相应的步骤。

1. 首先导入模块，如下。

```python
import torch
import torch.nn as nn

import bitsandbytes as bnb
from bnb.nn import Linear8bitLt
```

1. 然后就可以定义自己的模型了。请注意，我们支持将任何精度的 checkpoint 或模型转换为 8 位 (FP16、BF16 或 FP32)，但目前，仅当模型的输入张量数据类型为 FP16 时，我们的 Int8 模块才能工作。因此，这里我们称模型为 fp16 模型。

```python
fp16_model = nn.Sequential(
    nn.Linear(64, 64),
    nn.Linear(64, 64)
)
```

1. 假设你已经在你的数据集和任务上训完了你的模型！现在需要保存模型:

```python
[... train the model ...]
torch.save(fp16_model.state_dict(), "model.pt")
```

1. 至此，`state_dict` 已保存，我们需要定义一个 int8 模型:

```python
int8_model = nn.Sequential(
    Linear8bitLt(64, 64, has_fp16_weights=False),
    Linear8bitLt(64, 64, has_fp16_weights=False)
)
```

此处标志变量 `has_fp16_weights` 非常重要。默认情况下，它设置为 `True`，用于在训练时使能 Int8/FP16 混合精度。但是，因为在推理中我们对内存节省更感兴趣，因此我们需要设置 `has_fp16_weights=False`。

1. 现在加载 8 位模型！

```python
int8_model.load_state_dict(torch.load("model.pt"))
int8_model = int8_model.to(0) # 量化发生在此处
```

请注意，一旦将模型的设备设置为 GPU，量化过程就会在第二行代码中完成。如果在调用 `.to` 函数之前打印 `int8_model[0].weight`，你会看到:

```
int8_model[0].weight
Parameter containing:
tensor([[ 0.0031, -0.0438, 0.0494, ..., -0.0046, -0.0410, 0.0436],
        [-0.1013, 0.0394, 0.0787, ..., 0.0986, 0.0595, 0.0162],
        [-0.0859, -0.1227, -0.1209, ..., 0.1158, 0.0186, -0.0530],
        ...,
        [ 0.0804, 0.0725, 0.0638, ..., -0.0487, -0.0524, -0.1076],
        [-0.0200, -0.0406, 0.0663, ..., 0.0123, 0.0551, -0.0121],
        [-0.0041, 0.0865, -0.0013, ..., -0.0427, -0.0764, 0.1189]],
       dtype=torch.float16)
```

而如果你在第二行之后打印它，你会看到:

```
int8_model[0].weight
Parameter containing:
tensor([[ 3, -47, 54, ..., -5, -44, 47],
        [-104, 40, 81, ..., 101, 61, 17],
        [ -89, -127, -125, ..., 120, 19, -55],
        ...,
        [ 82, 74, 65, ..., -49, -53, -109],
        [ -21, -42, 68, ..., 13, 57, -12],
        [ -4, 88, -1, ..., -43, -78, 121]],
        device='cuda:0', dtype=torch.int8, requires_grad=True)
```

正如我们在前面部分解释量化方法时所讲，权重值被“截断”了。此外，这些值的分布看上去在 [-127, 127] 之间。

你可能还想知道如何获取 FP16 权重以便在 FP16 中执行离群值的矩阵乘？很简单:

```python
(int8_model[0].weight.CB * int8_model[0].weight.SCB) / 127
```

你会看到:

```
tensor([[ 0.0028, -0.0459, 0.0522, ..., -0.0049, -0.0428, 0.0462],
        [-0.0960, 0.0391, 0.0782, ..., 0.0994, 0.0593, 0.0167],
        [-0.0822, -0.1240, -0.1207, ..., 0.1181, 0.0185, -0.0541],
        ...,
        [ 0.0757, 0.0723, 0.0628, ..., -0.0482, -0.0516, -0.1072],
        [-0.0194, -0.0410, 0.0657, ..., 0.0128, 0.0554, -0.0118],
        [-0.0037, 0.0859, -0.0010, ..., -0.0423, -0.0759, 0.1190]],
       device='cuda:0')
```

这跟第一次打印的原始 FP16 值很接近！

1. 现在你只需将输入推给正确的 GPU 并确保输入数据类型是 FP16 的，你就可以使用该模型进行推理了:

```python
input_ = torch.randn(64, dtype=torch.float16)
hidden_states = int8_model(input_.to(torch.device('cuda', 0)))
```

你可以查看 [示例脚本](https://huggingface.co/blog/assets/96_hf_bitsandbytes_integration/example.py)，获取完整的示例代码！

多说一句， `Linear8bitLt` 与 `nn.Linear` 模块略有不同，主要在 `Linear8bitLt` 的参数属于 `bnb.nn.Int8Params` 类而不是 `nn.Parameter` 类。稍后你会看到这给我们带来了一些小麻烦！

现在我们开始了解如何将其集成到 `transformers` 库中！

## `accelerate` 足矣

在处理大模型时， `accelerate` 库包含许多有用的工具。`init_empty_weights` 方法特别有用，因为任何模型，无论大小，都可以在此方法的上下文 (context) 内进行初始化，而无需为模型权重分配任何内存。

```python
import torch.nn as nn
from accelerate import init_empty_weights

with init_empty_weights():
    model = nn.Sequential([nn.Linear(100000, 100000) for _ in range(1000)]) # This will take ~0 RAM!
```

初始化过的模型将放在 PyTorch 的 `meta` 设备上，这是一种用于表征向量的形状和数据类型而无需实际的内存分配的超酷的底层机制。

最初，我们在 `.from_pretrained` 函数内部调用 `init_empty_weights`，并将所有参数重载为 `torch.nn.Parameter`。这不是我们想要的，因为在我们的情况中，我们希望为 `Linear8bitLt` 模块保留 `Int8Params` 类，如上所述。我们最后成功使用 [此 PR](https://github.com/huggingface/accelerate/pull/519) 修复了该问题，它将下述代码:

```python
module._parameters[name] = nn.Parameter(module._parameters[name].to(torch.device("meta")))
```

修改成:

```python
param_cls = type(module._parameters[name])
kwargs = module._parameters[name].__dict__
module._parameters[name] = param_cls(module._parameters[name].to(torch.device("meta")), **kwargs)
```

现在这个问题已经解决了，我们可以轻松地在一个自定义函数中利用这个上下文管理器将所有 `nn.Linear` 模块替换为 `bnb.nn.Linear8bitLt` 而无需占用内存！

```python
def replace_8bit_linear(model, threshold=6.0, module_to_not_convert="lm_head"):
    for name, module in model.named_children():
        if len(list(module.children())) > 0:
            replace_8bit_linear(module, threshold, module_to_not_convert)

        if isinstance(module, nn.Linear) and name != module_to_not_convert:
            with init_empty_weights():
                model._modules[name] = bnb.nn.Linear8bitLt(
                    module.in_features,
                    module.out_features,
                    module.bias is not None,
                    has_fp16_weights=False,
                    threshold=threshold,
                )
    return model
```

此函数递归地将 `meta` 设备上初始化的给定模型的所有 `nn.Linear` 层替换为 `Linear8bitLt` 模块。这里，必须将 `has_fp16_weights` 属性设置为 `False`，以便直接将权重加载为 `Int8`，并同时加载其量化统计信息。

# 参考

1. [LLM（十一）：大语言模型的模型量化(INT8/INT4)技术](https://zhuanlan.zhihu.com/p/627436535)
2. [大规模 Transformer 模型 8 比特矩阵乘简介](https://huggingface.co/blog/zh/hf-bitsandbytes-integration)
3. [LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale](https://arxiv.org/abs/2208.07339)
	1. [中文版本翻译](https://yiyibooks.cn/arxiv/2208.07339v2/index.html)
