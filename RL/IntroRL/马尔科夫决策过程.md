
Markov Decision Process(MDP)是lecture2中详细讨论的一个建模框架，这里针对其中的课堂上没有深入的重要点做一些笔记。

---
# Fully Observed(完全可观察)

在强化学习（Reinforcement Learning, RL）中，Markov Decision Process（MDP）是一种数学框架，用于描述智能体（Agent）与环境（Environment）交互的过程。MDP 的定义包含四个核心元素：状态集合（States, $S$）、动作集合（Actions, $A$）、状态转移概率（Transition Probabilities, $P$）和奖励函数（Reward Function, $R$）。当我们说“在 MDP 中，环境是完全可观察的”（fully observable），**其实是指一个假设前提：智能体能够完全感知当前状态 $s \in S$，并且这个状态包含了所有与决策相关的信息**。

### 为什么 MDP 假设环境是完全可观察的？

这个“完全可观察”的特性来源于 MDP 的 **马尔可夫性质（Markov Property）**。马尔可夫性质表明，当前状态 $s_t$ 包含了所有历史信息中与未来状态 $s_{t+1}$ 和奖励 $r_t$ 相关的必要信息。换句话说，给定当前状态 $s_t$，未来的状态转移和奖励只依赖于 $s_t$ 和当前动作 $a_t$，而不依赖于更早的历史状态或动作。即：
$$
P(s_{t+1}, r_t | s_t, a_t) = P(s_{t+1}, r_t | s_0, a_0, s_1, a_1, \dots, s_t, a_t)
$$
这种性质简化了问题，使得智能体可以通过观察当前状态 $s_t$ 来做出最优决策，而无需回溯整个历史。

在这种情况下，“环境完全可观察”指的是智能体能够直接访问到状态 $s_t$。这意味着：

1. **无感知限制**：智能体不会因为感官限制或噪声而无法准确获取状态。
2. **状态充分性**：状态 $s_t$ 本身已经足够描述环境的所有相关特性，智能体不需要额外的隐藏信息来推断环境的行为。

### MDP 与现实的对比

需要注意的是，“环境完全可观察”只是 MDP 的一个理论假设。在实际应用中，很多强化学习问题并不满足这个条件。例如：

- **部分可观察环境（Partially Observable Environments）**：如果智能体无法直接访问完整状态，而是只能通过观测（Observations）来间接推测状态，就需要用到 **部分可观察马尔可夫决策过程（POMDP）**。比如，一个机器人可能只能通过传感器看到环境的局部信息，而不是全局状态。
- **噪声或不确定性**：现实环境中，状态可能受到噪声干扰，导致智能体无法完全信任观测到的信息。

但在标准的 MDP 框架下，为了简化建模和计算，假设环境是完全可观察的。这使得问题可以通过动态规划（如值迭代或策略迭代）或 Q-learning 等方法高效求解。

举一个简单的例子，假设你在玩一个迷宫游戏：

- 如果是 MDP：你能看到整个迷宫的地图，知道自己当前的位置（状态 $s_t$），并根据当前位置选择移动方向（动作 $a_t$）。环境是完全可观察的，因为状态 $s_t$（你的位置）包含了决策所需的所有信息。
- 如果不是 MDP（比如 POMDP）：你只能看到周围一小块区域（观测），而不知道自己在全局迷宫中的确切位置。这时环境就不是完全可观察的，你需要基于部分观测来推断状态。

### MDP 可以建模的问题类型

MDP 适用于那些可以用状态、动作、奖励和状态转移来描述的序贯决策问题（Sequential Decision Problems）。它的核心假设是马尔可夫性质，即“未来只依赖于当前状态和动作”。在理论上，只要问题可以被抽象成这种形式，MDP 就能够建模。以下是一些典型例子：

#### 1. 游戏和模拟环境

- **例子**：棋类游戏（如国际象棋、围棋）、迷宫导航、简单视频游戏（如 Atari 游戏）。
- **为什么适合 MDP**：在这些环境中，状态（棋盘布局、玩家位置）是完全可观察的，动作（移动棋子、按键）直接影响下一步状态和奖励（得分或输赢）。例如，国际象棋中，当前棋盘状态决定了所有可能的未来走法和结果。
- **现实近似**：即使现实中玩家可能有心理博弈或隐藏策略，MDP 仍然可以通过定义状态为“当前棋盘”来有效建模。

#### 2. 机器人控制

- **例子**：机械臂抓取物体、无人机导航。
- **为什么适合 MDP**：状态可以定义为机器人当前的传感器数据（如位置、速度、方向），动作是控制指令（如加速、转向），奖励是任务完成度（如成功抓取）。如果传感器数据足够丰富，状态可以近似为完全可观察。
- **现实近似**：现实中可能有噪声或不可见障碍，但通过状态设计（例如加入历史数据或传感器融合），可以让问题接近 MDP。

#### 3. 资源管理和调度

- **例子**：库存管理、任务调度、能源分配。
- **为什么适合 MDP**：状态可以是当前的资源水平（如库存量、任务队列），动作是分配决策（如订货、优先级调整），奖励是收益或成本。状态转移可能是随机的（需求波动），但当前状态通常足以描述系统。
- **现实近似**：虽然需求可能受长期趋势影响，但可以通过将时间窗口纳入状态来满足马尔可夫性质。

#### 4. 金融决策

- **例子**：股票交易、投资组合优化。
- **为什么适合 MDP**：状态可以是当前的资产价格和持有量，动作是买入或卖出，奖励是收益。价格变化可以用概率模型（如随机游走）表示。
- **现实近似**：金融市场显然受历史趋势和外部因素影响，但通过将近期价格序列或市场指标纳入状态，MDP 可以近似建模。

#### 5. 医疗决策

- **例子**：治疗方案选择、药物剂量调整。
- **为什么适合 MDP**：状态是患者的当前健康指标（如血压、血糖），动作是治疗选择，奖励是健康改善程度。状态转移反映患者对治疗的响应。
- **现实近似**：患者病情可能有隐藏变量（如未检测的并发症），但通过丰富的状态表示（更多检测数据），可以逼近 MDP。

### 如何让现实问题适应 MDP？

现实世界非常复杂，不像游戏一样拥有诸多良好的状态性质，这确实是 MDP 在现实应用中的挑战。以下是几种常见方法，让非马尔可夫问题适配 MDP 框架：

1. **状态增强（State Augmentation）**
   - **方法**：将历史信息或额外上下文纳入状态定义。
   - **例子**：在导航任务中，如果当前坐标不足以判断方向，可以将“过去几步的坐标”加入状态，形成一个更高维的状态空间。这样，即使环境本身不是马尔可夫的，增强后的状态可以满足马尔可夫性质。

2. **假设简化**
   - **方法**：忽略次要的历史依赖，假设当前状态“足够好”。
   - **例子**：在天气预测中，未来天气可能依赖长期趋势，但如果只预测下一小时，可以假设当前温度和湿度足以决定结果。

3. **模型近似**
   - **方法**：用概率模型（如随机过程）近似状态转移。
   - **例子**：在金融交易中，价格变化可能受复杂因素驱动，但可以用简单的随机模型（如几何布朗运动）来近似描述。

4. **扩展到 POMDP**
   - **方法**：如果状态完全不可观察，就转为部分可观察马尔可夫决策过程（POMDP），通过观测和信念状态（Belief State）推断隐藏状态。
   - **例子**：扑克游戏中，玩家的手牌是隐藏的，但可以通过对手的行为更新信念状态。

### MDP 的局限性与现实的差距

尽管 MDP 很强大，但它确实有局限性，尤其在以下情况：

- **隐藏状态**：如你所说，现实中很多问题有未观察到的变量（如人的意图、自然界的潜在规律），这使得纯 MDP 不够。
- **非平稳性（Non-Stationarity）**：**环境可能随时间变化（如规则改变）**，而 MDP 假设状态转移概率固定（假设环境是不会变化的）。
- **计算复杂性**：在状态空间很大时（如连续状态），MDP 的求解变得困难，需要用到函数逼近（如深度强化学习）。

因此，在实践中，强化学习研究者往往结合 MDP 和其他方法（如深度学习、POMDP、元学习）来应对复杂问题。例如，AlphaGo 虽然基于 MDP 思想，但通过神经网络预测状态价值和策略，大幅提高了对围棋这种高维问题的建模能力。
### 总结

MDP 可以建模的是一类“状态-动作-奖励”结构清晰、当前状态能较好表征未来的问题，如游戏、机器人控制、资源管理等。现实中完全满足马尔可夫性质的问题确实不多，但通过状态设计、近似假设或扩展框架（如 POMDP），MDP 仍然是强化学习的核心工具。你的质疑很有道理，这也正是强化学习领域不断发展的动力——从 MDP 到更复杂的模型，以适应真实世界的多样性。

---
# Policy Evaluation(策略评估)

策略评估算法中有一个重要而且有意思的细节，首先看看policy evaluation的算法：

在每次迭代 $t+1$ 时，更新所有状态 $s \in S$ 的 $v_{t+1}(s)$，其中 $s'$ 是 $s$ 的后继状态：

$$
v_{t+1}(s) = \sum_{a \in A} \pi(a|s)\left(R(s, a) + \gamma \sum_{s' \in S} P(s'|s, a)v_t(s')\right)
$$

不仔细看，会以为这和value function的预期未来回报的定义有冲突，因为value function的定义就是从后继的状态中计算所有奖励值，而这个公式中展现出来的是一种前向计算的过程。实际上，可以发现这个方向其实没有关系，因为$v_{t+1}是从v_{t}$来计算的，而$s\rightarrow s'$，说明这本质上是相同的！

---
# Optimal Value Function and Policy

>There exists a unique optimal value function, but could be multiple optimal policies (two actions that have the same optimal value function)

### 1. **什么是 MDP 和 optimal value function？**

在马尔可夫决策过程（MDP, Markov Decision Process）中，我们的目标是通过选择一系列动作（policy）来最大化累积奖励（通常是折扣后的累积奖励）。为了评估某个状态或状态-动作对的价值，我们定义了值函数（value function），主要有两种：

- **状态值函数（$V(s)$）**：表示在状态 $s$ 下，遵循某个策略所能获得的最优期望回报。
- **动作值函数（$Q(s, a)$）**：表示在状态 $s$ 下，采取动作 $a$ 后，遵循某个策略所能获得的最优期望回报。

“Optimal value function”指的是最优值函数，即在所有可能策略中，找到能够最大化累积奖励的那个值函数。通常我们讨论的是最优状态值函数 $V^*(s)$ 或最优动作值函数 $Q^*(s, a)$。在 MDP 中，最优值函数是唯一的，因为它是由 MDP 的状态转移、奖励函数和折扣因子唯一确定的数学性质。

### 2. **为什么 optimal value function 只有一个？**

最优值函数（比如 $V^*(s)$）是基于 MDP 的数学结构唯一确定的。它的定义是：
$$
V^*(s) = \max_{\pi} \mathbb{E} \left[ \sum_{t=0}^{\infty} \gamma^t R_t \mid S_0 = s, \pi \right]
$$
其中：
- $s$ 是某个状态，
- $\pi$ 是策略，
- $\gamma$ 是折扣因子（$0 \leq \gamma < 1$），
- $R_t$ 是时间 $t$ 的奖励。

这个公式表明，最优值函数 $V^*(s)$ 是所有可能策略中能够获得最大期望回报的值。MDP 的状态空间、动作空间、转移概率和奖励函数都是固定的，因此最优值函数的计算结果是唯一的。不存在两个不同的最优值函数，因为它们会违背 MDP 中“最优”的定义（即最大化累积奖励）。

**换句话说，最优值函数是 MDP 的固有属性，它不依赖于具体的策略选择，而只依赖于 MDP 的结构和参数**。(可以很容易证明，value function的最优本质上是由于optimal policy决定的，它不在乎optimal policy的运作，只在乎最后的policy evaluation的价值)

### 3. **为什么 optimal policy 可能有多个？**

虽然最优值函数是唯一的，但最优策略（optimal policy）却可能有多个。这是因为不同的策略可能导致相同的最大期望回报（即相同的 $V^*(s)$），但这些策略的具体动作选择可能不同。

在 MDP 中，一个策略 $\pi$ 被认为是“最优的”，如果它能使每个状态 $s$ 的值函数等于 $V^*(s)$。然而，在某些状态下，可能存在多个动作 $a_1, a_2, \dots, a_n$，它们在当前状态下的 $Q^*(s, a)$ 值都等于 $V^*(s)$。这意味着选择这些动作中的任何一个，都不会改变累积奖励的期望值，因此它们都是“最优的”。

例如：
- 假设你在状态 $s$ 有两个动作 $a_1$ 和 $a_2$，并且 $Q^*(s, a_1) = Q^*(s, a_2) = V^*(s)$。那么无论你选择 $a_1$ 还是 $a_2$，累积奖励的期望值都是相同的。因此，这两个动作都可以构成一个最优策略。
- 如果 MDP 有多个这样的状态和动作对，可能存在多个不同的最优策略（policy），但它们都共享同一个最优值函数 $V^*(s)$。

---
# Policy Iteration(代码实现)

[frozenlake_policy_iteration.py](https://github.com/ucla-rlcourse/RLexample/blob/master/MDP/frozenlake_policy_iteration.py)

```python
"""
Solving FrozenLake environment using Policy-Iteration.

Adapted by Bolei Zhou. Originally from Moustafa Alzantot (malzantot@ucla.edu)

updated from suggestions from ghost0832, Jan 3, 2025
"""
import gymnasium as gym
import numpy as np


def run_episode(env, policy, gamma=1.0):
    """ Runs an episode and return the total reward """
    obs, _ = env.reset()
    total_reward = 0
    step_idx = 0
    while True:
        obs, reward, terminated, truncated, _ = env.step(int(policy[obs]))
        done = np.logical_or(terminated, truncated)  # here use the logical or, one can use terminal
        total_reward += (gamma ** step_idx * reward)
        step_idx += 1
        if done:
            break
    return total_reward


def evaluate_policy(env, policy, gamma=1.0, n=100):
    scores = [run_episode(env, policy, gamma) for _ in range(n)]
    return np.mean(scores)


def extract_policy(v, gamma=1.0):
    """ Extract the policy given a value-function """
    policy = np.zeros(env.observation_space.n)
    for s in range(env.observation_space.n):
        q_sa = np.zeros(env.action_space.n)
        for a in range(env.action_space.n):
            q_sa[a] = sum([p * (r + gamma * v[s_]) for p, s_, r, _ in env.P[s][a]])
        policy[s] = np.argmax(q_sa)
    return policy


def compute_policy_v(env, policy, gamma=1.0):
    """ Iteratively evaluate the value-function under policy.
    Alternatively, we could formulate a set of linear equations in iterms of v[s] 
    and solve them to find the value function.
    """
    v = np.zeros(env.observation_space.n)
    eps = 1e-10
    while True:
        prev_v = np.copy(v)
        for s in range(env.observation_space.n):
            policy_a = policy[s]
            v[s] = sum([p * (r + gamma * prev_v[s_]) for p, s_, r, _ in env.P[s][policy_a]])
        if (np.sum((np.fabs(prev_v - v))) <= eps):
            # value converged
            break
    return v


def policy_iteration(env, gamma=1.0):
    """ Policy-Iteration algorithm """
    policy = np.random.choice(env.action_space.n, size=(env.observation_space.n))  # initialize a random policy
    max_iterations = 200000
    gamma = 1.0
    for i in range(max_iterations):
        old_policy_v = compute_policy_v(env, policy, gamma)
        new_policy = extract_policy(old_policy_v, gamma)
        if (np.all(policy == new_policy)):
            print('Policy-Iteration converged at step %d.' % (i + 1))
            break
        policy = new_policy
    return policy


if __name__ == '__main__':
    render = True
    env_name = 'FrozenLake-v1'  # 'FrozenLake8x8-v0'
    if render:
        env = gym.make(env_name, render_mode='human')
    else:
        env = gym.make(env_name)
    env = env.unwrapped
    optimal_policy = policy_iteration(env, gamma=1.0)
    scores = evaluate_policy(env, optimal_policy, gamma=1.0)
    print('Average scores = ', np.mean(scores))
```

实际上代码实现很容易懂，首先看一下核心的policy evaluation的部分的代码：

```python
 prev_v = np.copy(v)
for s in range(env.observation_space.n):
	policy_a = policy[s]
	v[s] = sum([p * (r + gamma * prev_v[s_]) for p, s_, r, _ in env.P[s][policy_a]])
```

其中对应实现的公式就是：
$$
v_{t+1}(s) = \sum_{a \in A} \pi(a|s)\left(R(s, a) + \gamma \sum_{s' \in S} P(s'|s, a)v_t(s')\right)
$$
然后就是提取其中的policy函数的部分：

```python
for s in range(env.observation_space.n):
	q_sa = np.zeros(env.action_space.n)
	for a in range(env.action_space.n):
		q_sa[a] = sum([p * (r + gamma * v[s_]) for p, s_, r, _ in env.P[s][a]])
	policy[s] = np.argmax(q_sa)
```

1. Compute the state-action value of a policy $\pi$:
   $$
   q^{\pi_i}(s, a) = R(s, a) + \gamma \sum_{s' \in S} P(s' | s, a) v^{\pi_i}(s')
   $$

2. Compute new policy $\pi_{i+1}$ for all $s \in S$ following:
   $$
   \pi_{i+1}(s) = \arg\max_a q^{\pi_i}(s, a)
   $$
---
# Value Iteration(代码实现)

[frozenlake_value_iteration.py](https://github.com/ucla-rlcourse/RLexample/blob/master/MDP/frozenlake_value_iteration.py)

```python
"""
Solving FrozenLake environment using Value-Iteration.

Updated 17 Aug 2020

updated by Bolei from the feedback of ghost0832, Jan 3, 2025
"""
import gymnasium as gym
import numpy as np


def run_episode(env, policy, gamma=1.0):
    """ Evaluates policy by using it to run an episode and finding its
    total reward.

    args:
    env: gym environment.
    policy: the policy to be used.
    gamma: discount factor.
    render: boolean to turn rendering on/off.

    returns:
    total reward: real value of the total reward received by agent under policy.
    """
    obs, _ = env.reset()
    total_reward = 0
    step_idx = 0
    while True:
        obs, reward, terminated, truncated, _ = env.step(int(policy[obs]))
        done = np.logical_or(terminated, truncated)  # here use the logical or, one can use terminal
        total_reward += (gamma ** step_idx * reward)
        step_idx += 1
        if done:
            break
    return total_reward


def evaluate_policy(env, policy, gamma=1.0, n=100):
    """ Evaluates a policy by running it n times.
    returns:
    average total reward
    """
    scores = [
        run_episode(env, policy, gamma=gamma)
        for _ in range(n)]
    return np.mean(scores)


def extract_policy(v, gamma=1.0):
    """ Extract the policy given a value-function """
    policy = np.zeros(env.observation_space.n)
    for s in range(env.observation_space.n):
        q_sa = np.zeros(env.action_space.n)
        for a in range(env.action_space.n):
            for next_sr in env.P[s][a]:
                # next_sr is a tuple of (probability, next state, reward, done)
                p, s_, r, _ = next_sr
                q_sa[a] += (p * (r + gamma * v[s_]))
        policy[s] = np.argmax(q_sa)
    return policy


def value_iteration(env, gamma=1.0):
    """ Value-iteration algorithm """
    v = np.zeros(env.observation_space.n)  # initialize value-function
    max_iterations = 100000
    eps = 1e-20
    for i in range(max_iterations):
        prev_v = np.copy(v)
        for s in range(env.observation_space.n):
            q_sa = [sum([p * (r + gamma * prev_v[s_]) for p, s_, r, _ in env.P[s][a]]) for a in
                    range(env.action_space.n)]
            v[s] = max(q_sa)
        if (np.sum(np.fabs(prev_v - v)) <= eps):
            print('Value-iteration converged at iteration# %d.' % (i + 1))
            break
    return v


if __name__ == '__main__':
    render = True
    env_name = 'FrozenLake-v1'  # 'FrozenLake8x8-v0'
    if render:
        env = gym.make(env_name, render_mode='human')
    else:
        env = gym.make(env_name)
    env = env.unwrapped
    gamma = 1.0
    optimal_v = value_iteration(env, gamma)
    policy = extract_policy(optimal_v, gamma)
    policy_score = evaluate_policy(env, policy, gamma, n=1000)
    print('Policy average score = ', policy_score)
```

相比于policy iteration来讲，这个value iteration就更加简单了，因为都不需要显式提取出policy来就可以计算得到optimal value function。

```python
prev_v = np.copy(v)
for s in range(env.observation_space.n):
	q_sa = [sum([p * (r + gamma * prev_v[s_]) for p, s_, r, _ in env.P[s][a]]) for a in range(env.action_space.n)]
	v[s] = max(q_sa)
```

以上就是计算optimal value function的过程，对应下面的bellman optimality equation的优化方法：

$$q_{k+1}(s, a) = R(s, a) + \gamma \sum_{s' \in S} P(s'|s, a) v_k(s')$$
$$v_{k+1}(s) = \max_a q_{k+1}(s, a)$$

```python
for s in range(env.observation_space.n):
	q_sa = np.zeros(env.action_space.n)
	for a in range(env.action_space.n):
		for next_sr in env.P[s][a]:
			# next_sr is a tuple of (probability, next state, reward, done)
			p, s_, r, _ = next_sr
			q_sa[a] += (p * (r + gamma * v[s_]))
	policy[s] = np.argmax(q_sa)
```

这部分的policy抽取的逻辑和policy iteration也是很像的：

$$\pi(s) = \arg\max_a R(s, a) + \gamma \sum_{s' \in S} P(s'|s, a) v_{k+1}(s')$$
总得来看，代码并不复杂，主要就是要理解动态规划中的状态转移即可。