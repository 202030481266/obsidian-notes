# jieba 分词工具原理

`jieba` 是一个中文分词工具，其分词原理主要基于以下几个关键点：

---

## 1. 基于词典的最大概率匹配

- **核心思想**：利用词典中的词频信息，通过动态规划算法，找到一段文字中词语划分的最大概率路径。
- **算法**：`jieba` 实现了基于**前缀树**和**动态规划**的分词方法。
  1. 将待分词文本划分为多个可能的分词路径（每个路径是一种可能的分词结果）。
  2. 对每条路径计算总的词频概率（取对数累加以避免下溢出）。
  3. 选择总概率最大的路径作为分词结果。
- **数学原理**：  
  对于输入文本 $S = \{w_1, w_2, ..., w_n\}$，分词结果的概率为：  
  $$
  P(S) = P(w_1) \cdot P(w_2) \cdot ... \cdot P(w_n)
  $$  
  使用对数优化为：  
  $$
  \text{log}P(S) = \text{log}P(w_1) + \text{log}P(w_2) + ... + \text{log}P(w_n)
  $$  
  `jieba` 使用动态规划快速求解，使得分词结果具有最大可能性。

---

## 2. 利用自定义词典

- 支持加载自定义词典，增强分词能力。
- 自定义词典中的词语可以带有词频信息，用于影响分词的优先级。例如，较高的词频会让该词语更可能出现在最终分词结果中。

---

## 3. HMM（隐马尔可夫模型）

- 内部使用隐马尔可夫模型（HMM）处理未登录词（即词典中不存在的词语）。
- **核心思想**：将中文分词问题建模为一个序列标注问题，通过观察到的字符序列，推断出每个字符的状态（即是词的开始、中间、结尾或独立成词）。
- **模型训练**：
  1. 状态集合：`B`（词首）、`M`（词中）、`E`（词尾）、`S`（单字成词）。
  2. 特征集合：基于字符上下文关系。
  3. 通过已标注的语料库训练出转移概率和发射概率。
- **分词过程**：给定输入序列，利用维特比算法（Viterbi Algorithm）找到最优的状态序列，从而确定分词位置。

更加详细的内容：[[HMM]]

---

## 4. 全模式、精确模式与搜索引擎模式

提供三种分词模式：
1. **全模式**：将句子中所有可能的词语都扫描出来，速度快，但不能解决歧义。示例：  

我来到北京清华大学 → [我, 来, 到, 北京, 清华, 清华大学, 华大, 大学]

2. **精确模式**：试图以最精确的方式切分句子，适合文本分析。示例： 

我来到北京清华大学 → [我, 来到, 北京, 清华大学]

3. **搜索引擎模式**：在精确模式的基础上，对较长词再进行切分，适合搜索引擎构建索引。示例：  

我来到北京清华大学 → [我, 来到, 北京, 清华, 华大, 大学, 清华大学]

---

## 5. 基于字典树和 Trie 结构的加速

- 内部使用 Trie 树存储词典，快速查找前缀匹配的候选词。
- Trie 树是一种高效的字符串匹配数据结构，能将多个字符串以前缀共享的形式存储，从而减少存储空间和查找时间。

---

## 6. 停用词处理

- 在一些应用场景（如文本分析、信息检索）中，支持停用词过滤功能，用于去除对分词结果无实际意义的高频词，如“的”“了”“在”等。

---

## 7. 拓展功能

- **新词发现**：通过分析大文本中的共现频率（如 PMI）和左右熵判断新词的合理性。
- **支持自定义调整**：允许动态调整分词策略（如强制加入新词、调整词语权重等）。

---

## 示例代码


```python
import jieba

# 精确模式分词
text = "我来到北京清华大学"
words = jieba.cut(text, cut_all=False)
print("精确模式:", "/".join(words))

# 全模式分词
words = jieba.cut(text, cut_all=True)
print("全模式:", "/".join(words))

# 搜索引擎模式分词
words = jieba.cut_for_search(text)
print("搜索引擎模式:", "/".join(words))

输出：

```

精确模式: 我/来到/北京/清华大学
全模式: 我/来到/北京/清华/清华大学/华大/大学
搜索引擎模式: 我/来到/北京/清华/华大/大学/清华大学

---

## 总结

`jieba` 的核心技术结合了词典匹配、动态规划、HMM、以及灵活的自定义扩展，为中文分词提供了高效且易用的解决方案。