# 乘法技巧1

## 优化原理

当需要执行多个具有相同第一操作数的矩阵乘法操作时，可以将第二操作数在最后一个维度上拼接，执行一次矩阵乘法后再分割结果，从而减少计算开销。

## 形式化描述

假设我们有以下操作：

- 矩阵 A 形状为 [m, n]
- 矩阵 B 形状为 [n, p]
- 矩阵 C 形状为 [n, q]

我们需要计算:

- `Result_B = A @ B` 形状为 [m, p]
- `Result_C = A @ C` 形状为 [m, q]

**传统方法**需要两次独立的矩阵乘法操作：

1. `Result_B = A @ B`
2. `Result_C = A @ C`

**批处理优化方法**：

1. 将B和C在最后一个维度拼接: `D = cat([B, C], dim=1)` 形状为 [n, p+q]
2. 一次计算: `Result_D = A @ D` 形状为 [m, p+q]
3. 分割结果: `Result_B, Result_C = split(Result_D, [p, q], dim=1)`

## 计算复杂度分析

传统方法:

- 第一次乘法: O(m·n·p)
- 第二次乘法: O(m·n·q)
- 总计算量: O(m·n·p + m·n·q) = O(m·n·(p+q))

批处理方法:

- 拼接操作: O(n·(p+q))
- 单次乘法: O(m·n·(p+q))
- 分割操作: O(m·(p+q))
- 总计算量: O(m·n·(p+q) + n·(p+q) + m·(p+q))

两种方法的渐近计算复杂度相同，但批处理方法减少了内核启动次数和内存访问，提高了并行效率。

## 代码实现

```python
import torch
import time

# 设置随机种子以保证结果可重现
torch.manual_seed(42)

def benchmark(func, name, iterations=100):
    """测量函数的平均执行时间"""
    start = time.time()
    for _ in range(iterations):
        result = func()
    end = time.time()
    print(f"{name} 平均耗时: {(end - start) / iterations * 1000:.4f} ms")
    return result

# 例子1: 标准矩阵乘法
def example_standard():
    # 创建测试矩阵
    A = torch.randn(128, 256, device='cuda')
    B = torch.randn(256, 512, device='cuda')
    C = torch.randn(256, 384, device='cuda')
    
    # 基准方法: 分别执行两次矩阵乘法
    def baseline():
        result_B = A @ B
        result_C = A @ C
        return result_B, result_C
    
    # 优化方法: 拼接后一次乘法，再分割结果
    def optimized():
        D = torch.cat([B, C], dim=1)
        result = A @ D
        return torch.split(result, [512, 384], dim=1)
    
    # 验证结果相同
    result_baseline = baseline()
    result_optimized = optimized()
    
    print("结果是否相同:", all(torch.allclose(b, o) for b, o in zip(result_baseline, result_optimized)))
    
    # 性能比较
    print("\n== 标准矩阵乘法性能比较 ==")
    benchmark(baseline, "基准方法")
    benchmark(optimized, "优化方法")

# 例子2: 批量矩阵乘法 (适用于Transformer模型中)
def example_batched():
    # 创建测试张量 (batch_size, seq_len, hidden_dim)
    batch_size, seq_len, hidden_dim = 32, 128, 768
    head_dim = 64
    num_heads = 12
    
    queries = torch.randn(batch_size, num_heads, seq_len, head_dim, device='cuda')
    keys = torch.randn(batch_size, num_heads, seq_len, head_dim, device='cuda')
    values = torch.randn(batch_size, num_heads, seq_len, head_dim, device='cuda')
    
    # 基准方法: 分别计算attention分数和加权值
    def baseline():
        # 计算attention分数 (batch_size, num_heads, seq_len, seq_len)
        attention = torch.matmul(queries, keys.transpose(-2, -1)) / (head_dim ** 0.5)
        attention = torch.softmax(attention, dim=-1)
        
        # 计算输出 (batch_size, num_heads, seq_len, head_dim)
        output = torch.matmul(attention, values)
        return output
    
    # 优化方法: 将keys和values拼接进行一次计算
    def optimized():
        # 计算attention分数
        attention = torch.matmul(queries, keys.transpose(-2, -1)) / (head_dim ** 0.5)
        attention = torch.softmax(attention, dim=-1)
        
        # 同时计算多个头的结果
        # 此处假设我们要将多个头的计算合并
        attention_multi = attention.reshape(batch_size, num_heads*seq_len, seq_len)
        values_multi = values.reshape(batch_size, num_heads, seq_len*head_dim)
        
        # 优化: 多个头的值一起计算
        output_multi = torch.bmm(attention_multi, values.reshape(batch_size, seq_len, num_heads*head_dim))
        output = output_multi.reshape(batch_size, num_heads, seq_len, head_dim)
        return output
    
    # 性能比较
    print("\n== 批量矩阵乘法性能比较 ==")
    benchmark(baseline, "基准方法")
    benchmark(optimized, "优化方法")

# 运行例子
if torch.cuda.is_available():
    example_standard()
    example_batched()
else:
    print("CUDA不可用，请在GPU环境中运行以获得准确的性能比较")
```