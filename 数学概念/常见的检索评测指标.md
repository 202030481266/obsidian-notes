
# 检索评测指标详解

在信息检索中，为了评价一个系统是否能够高效准确地找到相关内容，我们需要一些标准化的评估方法。这些评估指标主要包括 **MAP（Mean Average Precision）**、**MRR（Mean Reciprocal Rank）**、**Precision@k** 和 **Recall@k** 等，以下是对它们的简单解释和数学公式。

## 1. MAP（Mean Average Precision）

**MAP** 是用来衡量检索系统总体性能的指标，它先计算每个查询的平均精度（AP），再求所有查询的平均值。

### 平均精度（AP）

对于单个查询，AP 的定义是：

$$
AP =\frac{1}{R}\sum_{k=1}^nP(k)⋅\operatorname{rel}(k)
$$

- $R$：相关文档的总数。
- $n$：检索到的文档数量。
- $P(k)$：在前 $k$ 个结果中的精度。
	$$
	P(k)=\frac{1}{k}\sum_{i=1}^k\operatorname{rel}(i)
	$$
- $\text{rel}(k)$：第 $k$ 个结果是否相关，相关为 $1$，不相关为 $0$。

### MAP 的定义

所有查询的平均精度计算公式为：

$$MAP = \frac{1}{|Q|} \sum_{q=1}^{|Q|} AP(q)$$

- $|Q|$：查询的总数。
- $AP(q)$：第 $q$ 个查询的平均精度。

## 2. MRR（Mean Reciprocal Rank）

**MRR** 主要用来衡量第一个相关文档在检索结果中的排名，排名越靠前，分数越高。

### Reciprocal Rank

对于一个查询，第一个相关文档的倒数排名计算为：

$$RR = \frac{1}{\text{rank}}$$

- $\text{rank}$：第一个相关文档的排名位置。

### MRR 的定义

所有查询的 Reciprocal Rank 的平均值：

$$MRR = \frac{1}{|Q|} \sum_{q=1}^{|Q|} RR(q)$$

- $RR(q)$：第 $q$ 个查询的 Reciprocal Rank。

## 3. Precision@k

**Precision@k** 衡量的是前 $k$ 个检索结果中，相关文档的比例，强调检索结果的准确性。

### 数学定义

$$P@k = \frac{1}{k} \sum_{i=1}^k \text{rel}(i)$$

- $k$：取前 $k$ 个检索结果。
- $\text{rel}(i)$：第 $i$ 个结果是否相关。

## 4. Recall@k

**Recall@k** 衡量的是在前 $k$ 个检索结果中找到了多少个相关文档，与总的相关文档数量相比。

### 数学定义

$$R@k = \frac{\text{Number of relevant documents in top } k}{\text{Total number of relevant documents}}$$

或表示为：

$$R@k = \frac{1}{R} \sum_{i=1}^k \text{rel}(i)$$

- $R$：该查询的总相关文档数量。
- $\text{rel}(i)$：第 $i$ 个结果是否相关。

## 5. 示例

假设某个查询的检索结果和相关性标注如下：

|排名位置|相关性|计算 Precision@k|
|---|---|---|
|1|1|$P@1 = 1/1 = 1.0$|
|2|0|$P@2 = 1/2 = 0.5$|
|3|1|$P@3 = 2/3 \approx 0.67$|
|4|0|$P@4 = 2/4 = 0.5$|
|5|1|$P@5 = 3/5 = 0.6$|

## 6. F1-Score

**F1-Score** 是衡量模型精确性（Precision）和召回率（Recall）之间平衡的重要指标，尤其适用于需要在二者之间找到权衡的场景。

### 数学定义

 **True Positive (TP)**：
    - 指的是模型正确预测为正类（相关）的样本数量。
    - 举例：模型预测一篇文档相关且实际确实相关。
**False Positive (FP)**：
    - 指的是模型错误预测为正类（相关）的样本数量。
    - 举例：模型预测一篇文档相关，但实际是不相关的。
**False Negative (FN)**：
    - 指的是模型错误预测为负类（不相关）的样本数量。
    - 举例：模型预测一篇文档不相关，但实际是相关的。
**True Negative (TN)**：
    - 指的是模型正确预测为负类（不相关）的样本数量。
    - 举例：模型预测一篇文档不相关且实际确实不相关。

这些指标的组合用于计算精确性（Precision）和召回率（Recall），从而进一步计算 F1-Score：

- **Precision**：专注于模型预测的正类中有多少是正确的，公式为：
- **Recall**：专注于所有实际正类中被正确识别的比例，公式为：

其中：

- $\text{Precision} = \frac{\text{True Positives (TP)}}{\text{True Positives (TP)} + \text{False Positives (FP)}}$
    
- $\text{Recall} = \frac{\text{True Positives (TP)}}{\text{True Positives (TP)} + \text{False Negatives (FN)}}$
    

**F1-Score 是 Precision 和 Recall 的调和平均值**，当二者数值接近时，F1-Score 较高；当二者差距较大时，F1-Score 较低。