
## 参考文章

- [一个框架看懂优化算法之异同 SGD/AdaGrad/Adam](https://zhuanlan.zhihu.com/p/32230623)

## 优化器发展

### 优化器算法框架

感觉也不是很难的东西，动量感觉就是一种对梯度的处理，使用历史梯度+现在梯度计算出来一个折中的梯度，然后再进行优化，至于为什么这样做很有效，未解之谜。

待优化参数 $w$， 目标函数就是 $f(w)$，然后就是初始学习率为 $\alpha$。

然后开始迭代优化，在每个epoch $t$上：

1. 计算目标函数关于当前参数的梯度：$g_t=\nabla f(w_t)$
2. 根据历史梯度计算一阶的动量和二阶的动量：$m_t=\phi (g_1,g_2,...,g_t) ;  V_t=\psi (g_1,g_2,...,g_t)$
3. 计算当前时刻的下降梯度：$\eta_t = \alpha \cdot m_t / \sqrt{V_t}$
4. 根据梯度更新参数：$w_{t+1}=w_t-\eta_t$

所有的算法其实本质上都是这个框架。

### SGD

$$m_t=g_t; V_t = I^2$$

SGD的最大缺陷就是收敛缓慢并且很有可能陷入一个局部最优点。

### SGDM

SGDM（SGD with Momentum）是为了抑制SGD的震荡，其中主要思想就是利用了历史的梯度来决定**大体的收敛方向**而不是完全取决于现在的梯度。（上上上下 -> 上，而不是往下走，不会发生大幅变向的情况，从物理的视角来看，这其实就是惯性）

$$m_t=\beta_1 \cdot m_{t-1} + (1-\beta_1) \cdot g_t$$

其中 $\beta_1$ 的经验值就是 0.9。

### NAG

SGD依然无法有效解决陷入局部最优的问题。

NAG全称Nesterov Accelerated Gradient，是在SGD、SGDM的基础上的进一步改进，改进点在于步骤1。我们知道在时刻 $t$ 的主要下降方向是由累积动量决定的，自己的梯度方向说了也不算，那与其看当前梯度方向，不如先看看如果跟着累积动量走了一步，那个时候再怎么走（本质上就是防止在局部最优点来回震荡，因为有了动量的存在会越走越快，很容易走过头）。因此，NAG在步骤1，不计算当前位置的梯度方向，而是计算如果按照累积动量走了一步，那个时候的下降方向：

$$
g_t=\nabla f(w_t - \alpha \cdot m_{t-1} / \sqrt{V_{t-1}})
$$

### AdaGrad

开始进入了自适应的学习率优化算法时代。SGD及其变种以同样的学习率更新每个参数，但深度神经网络往往包含大量的参数，这些参数并不是总会用得到（想想大规模的embedding）。==对于经常更新的参数，我们已经积累了大量关于它的知识，不希望被单个样本影响太大，希望学习速率慢一些；对于偶尔更新的参数，我们了解的信息太少，希望能从每个偶然出现的样本身上多学一些，即学习速率大一些==。可以使用二阶动量去度量历史更新频率，也就是梯度值的平方和。

$$V_t = \sum_{\tau=1}^t g_{\tau}^2 $$

所以实际上学习率 $\alpha$ 变为了：
$$
\alpha \rightarrow \alpha / \sqrt{V_t}
$$

这个方法在稀疏数据下效果很好，但是会存在学习率提前减到0的现象，使得训练过程提前结束。

### AdaDelta 、 RMSProp

由于AdaGrad单调递减的学习率变化过于激进，我们考虑一个改变二阶动量计算方法的策略：不累积全部历史梯度，而只关注过去一段时间窗口的下降梯度。这也就是AdaDelta名称中Delta的来历。

修改的思路很简单。==指数移动平均值大约就是过去一段时间的平均值==，因此我们用这一方法来计算二阶累积动量：

$$V_t = \beta_2\ *\ V_{t-1} + (1-\beta_2)\ * \ g_t^2$$

这就避免了二阶动量持续累积、导致训练过程提前结束的问题了。

### Adam

在上述的优化器算法基础上，Adam和Nadam的出现就很自然而然了——它们是前述方法的集大成者。我们看到，SGD-M在SGD基础上增加了一阶动量，AdaGrad和AdaDelta在SGD基础上增加了二阶动量。把一阶动量和二阶动量都用起来，就是Adam了——Adaptive + Momentum。

SGD的一阶动量：

$$m_t=\beta_1 \cdot m_{t-1} + (1-\beta_1) \cdot g_t$$

AdaDelta的二阶动量：

$$V_t = \beta_2\ *\ V_{t-1} + (1-\beta_2)\ * \ g_t^2$$

这就是最常见的优化器算法。

### AdamW

在AdamW提出之前，Adam算法已经被广泛应用于深度学习模型训练中。但是人们发现，理论上更优的Adam算法，有时表现并不如SGD momentum好，尤其是在模型泛化性上。

我们知道，L2范数（也叫权重衰减weight decay）有助于提高模型的泛化性能。

但是AdamW的作者证明，Adam算法弱化了L2范数的作用，所以导致了用Adam算法训练出来的模型泛化能力较弱。具体来说，在Adam中，权重衰减的梯度是直接加在 $g_t$ 上的，这就导致权重衰减的梯度也会随着 $g_t$ 去除以分母。**当梯度的平方和累积过大时，权重衰减的作用就会被大大削弱**!

$$
\theta_t \leftarrow \theta_{t-1} - \alpha \ *\ \frac{\beta_1 m_{t-1} + (1-\beta_1)(\nabla f_t + \lambda \theta_{t-1})}{\sqrt{V_t} + \epsilon} 
$$
那么AdamW的做法就是解耦这两个梯度以及更新：

$$ \begin{aligned} \text{Initialize:} & \quad \theta_0, \, m_0 = 0, \, v_0 = 0, \, t = 0 \\ \text{While} & \quad \theta_t \text{ not converged do:} \\ & \quad t = t + 1 \\ & \quad g_t = \nabla_\theta L(\theta_{t-1}) \\ & \quad m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t \\ & \quad v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2 \\ & \quad \hat{m}_t = \frac{m_t}{1 - \beta_1^t} \\ & \quad \hat{v}_t = \frac{v_t}{1 - \beta_2^t} \\ & \quad \theta_t = \theta_{t-1} - \eta \left( \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon} + \lambda \theta_{t-1} \right) \end{aligned} $$

### 指数移动平均值偏差修正

在实际使用的过程中，经验值 $\beta_1=0.9, \beta_2=0.999$，初始化 $m_0=0,V_0=0$，然后而，这在初期会导致动量都是接近于0的，所以这个估计有问题，要进行误差修正：

$$
\begin{align}
\tilde{m_t} &= m_t / (1-\beta_1^t) \\
\tilde{V_t} &= V_t / (1-\beta_2^t)
\end{align}
$$



## Adam的诟病

其实可以概括为Adam是一种“无脑”的选择，但是想要达到最好的效果，精细化调优肯定是更好的，所以很多大神返璞归真，选择SGD优化算法进行调优，而不是直接Adam。下面是一些常见的Adam优化算法遇到的问题。

### 可能不会收敛

其中，SGD没有用到二阶动量，因此学习率是恒定的（实际使用过程中会采用学习率衰减策略，因此学习率递减）。AdaGrad的二阶动量不断累积，单调递增，因此学习率是单调递减的。因此，这两类算法会使得学习率不断递减，最终收敛到0，模型也得以收敛。

但AdaDelta和Adam则不然。二阶动量是固定时间窗口内的累积，随着时间窗口的变化，遇到的数据可能发生巨变，使得 $V_t$ 可能会时大时小，不是单调变化。这就可能在训练后期引起学习率的震荡，导致模型无法收敛。然后其实有论文给出了下面的修改：（保证了单调性）

$$
V_t=\max (\beta_2*V_{t-1}+(1-\beta_2)g_t^2, V_{t-1})
$$

这样的修改会使得 $\|V_t\| \ge \|V_{t-1}\|$ ，从而使得学习率衰减。

### 可能错过全局最优解

[深度神经网络](https://zhida.zhihu.com/search?q=%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C)往往包含大量的参数，在这样一个维度极高的空间内，非凸的目标函数往往起起伏伏，拥有无数个高地和洼地。有的是高峰，通过引入动量可能很容易越过；但有些是高原，可能探索很多次都出不来，于是停止了训练。

近期Arxiv上的两篇文章谈到这个问题。

[第一篇](https://zhida.zhihu.com/search?q=%E7%AC%AC%E4%B8%80%E7%AF%87)就是前文提到的吐槽Adam最狠的 [The Marginal Value of Adaptive Gradient Methods in Machine Learning](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1705.08292) 。文中说到，同样的一个优化问题，不同的优化算法可能会找到不同的答案，但自适应学习率的算法往往找到非常差的答案。他们通过一个特定的数据例子说明，自适应学习率算法可能会对前期出现的特征[过拟合](https://zhida.zhihu.com/search?q=%E8%BF%87%E6%8B%9F%E5%90%88)，后期才出现的特征很难纠正前期的拟合效果。

另外一篇是 [Improving Generalization Performance by Switching from Adam to SGD](https://link.zhihu.com/?target=http%3A//arxiv.org/abs/1712.07628)，进行了实验验证。他们CIFAR-10数据集上进行测试，Adam的收敛速度比SGD要快，但最终收敛的结果并没有SGD好。他们进一步实验发现，主要是后期Adam的学习率太低，影响了有效的收敛。他们试着对Adam的学习率的下界进行控制，发现效果好了很多。

于是他们提出了一个用来改进Adam的方法：前期用Adam，享受Adam快速收敛的优势；后期切换到SGD，慢慢寻找最优解。这一方法以前也被研究者们用到，不过主要是根据经验来选择切换的时机和切换后的学习率。这篇文章把这一切换过程傻瓜化，给出了切换SGD的时机选择方法，以及学习率的计算方法，效果看起来也不错。