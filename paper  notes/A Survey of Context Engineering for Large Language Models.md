2025/7/21 ——shulin
# 定义

上下文工程这个词最近太火热，许多人认为它是从Prompt Enginerring扩展而来的一个更大层面上的概念，但是界限还是模糊的。直到Wiki上有了对此概念的定义[Context Engineering](https://en.wikipedia.org/wiki/Prompt_engineering)。Github上也有大佬开放了一些关于上下文工程的库：

- [Context Engineering](https://github.com/davidkimai/Context-Engineering)，3.2k stars
- [Awesome-Context-Engineering](https://github.com/Meirtz/Awesome-Context-Engineering)，本期论文的代码仓库

这里采用本篇系统综述所采用的形式化定义。

---
## 形式化定义

为了正式定义上下文工程，我们从自回归大语言模型的标准概率模型开始。该模型由 θ 参数化，在给定输入上下文 C 的情况下，通过最大化条件概率来生成输出序列 Y = (y₁, ..., yT)：

$$P_\theta(Y|C) = \prod_{t=1}^{T} P_\theta(y_t|y_{<t}, C) \quad (1)$$

历史上，在提示工程的范式中，上下文 C 被视为一个单一的、静态的文本字符串，即 C = 提示。这种观点对于现代系统来说是不够的。

上下文工程将上下文 C 重新概念化为一个动态结构化的信息组件集合 c₁, c₂, ..., cn。这些组件由一组函数进行溯源、筛选和格式化，并最终由一个高级组装函数 A 进行编排：

$$C = A(c_1, c_2, \dots, c_n) \quad (2)$$

组件 cᵢ 不是任意的；它们直接映射到本调查的核心技术领域：
* **cᵢnstr**：系统指令和规则（上下文检索和生成，第 4.1 节）。
* **cₖnow**：外部知识，通过像 RAG 这样的函数或从集成的知识图中检索（RAG，第 5.1 节；上下文处理，第 4.2 节）。
* **cₜools**：可用外部工具的定义和签名（函数调用和工具集成推理，第 5.3 节）。
* **cₘem**：来自先前交互的持久信息（记忆系统，第 5.2 节；上下文管理，第 4.3 节）。
* **cₛtate**：用户、世界或多智能体系统的动态状态（多智能体系统与编排，第 5.4 节）。
* **cquery**：用户的即时请求。

---

## **上下文工程的优化问题**

从这个角度来看，上下文工程是一个形式化的优化问题，旨在找到一组理想的上下文生成函数（我们统称为 $\mathcal{F} = \{A, \text{Retrieve}, \text{Select}, \dots\}$），以最大化语言模型（LLM）输出的期望质量。给定任务分布 $\mathcal{T}$，其目标是：

$$\mathcal{F}^* = \arg\max_{\mathcal{F}} \mathbb{E}_{\tau \sim \mathcal{T}}[\text{Reward}(P_\theta(Y|C_{\mathcal{F}}(\tau)), Y^*_\tau)] \quad (3)$$

其中，$\tau$ 是一个具体的任务实例，$C_{\mathcal{F}}(\tau)$ 是由 $\mathcal{F}$ 中的函数为该任务生成的上下文，$Y^*_\tau$ 是基准真相或理想输出。此优化受到严格的约束，最主要的是模型的上下文长度限制，即 $|C| \le L_{\text{max}}$。

---
## **数学原理和理论框架**

这种形式化揭示了更深层次的数学原理。组装函数 $A$ 是一种**动态上下文编排（Dynamic Context Orchestration）的形式，它是一个由格式化和连接操作组成的流水线，$A = \text{Concat}(\text{Format}_1, \dots, \text{Format}_n)$，其中每个函数都必须针对LLM的架构偏差（例如，注意力模式）进行优化。

知识的检索，$c_{\text{know}} = \text{Retrieve}(\dots)$，可以被构建为一个**信息论最优性（Information-Theoretic Optimality）问题。其目标是，在给定查询 $c_{\text{query}}$ 的情况下，选择能够最大化与目标答案 $Y^*$ 之间互信息的知识：

$$\text{Retrieve}^* = \arg\max_{\text{Retrieve}} I(Y^*; c_{\text{know}}|c_{\text{query}}) \quad (4)$$

这确保了检索到的上下文不仅在语义上相似，而且对于解决任务具有最大的信息量。

此外，整个过程可以从**贝叶斯上下文推断（Bayesian Context Inference）的角度来审视。我们不是确定性地构建上下文，而是推断出最优的上下文后验概率 $P(C|c_{\text{query}}, \text{History}, \text{World})$。根据贝叶斯定理，这个后验概率正比于给定上下文时查询的似然度与上下文相关性的先验概率的乘积：

$$P(C|c_{\text{query}}, \dots) \propto P(c_{\text{query}}|C) \cdot P(C|\text{History}, \text{World}) \quad (5)$$

然后，决策论的目标是找到上下文 $C^*$，以最大化在所有可能答案分布下的期望奖励：

$$C^* = \arg\max_C \int P(Y|C, c_{\text{query}}) \cdot \text{Reward}(Y, Y^*) \,dY \cdot P(C|c_{\text{query}}, \dots) \quad (6)$$

这种贝叶斯公式提供了一种有原则的方法来处理不确定性，通过更新先验来执行自适应检索，并在多步推理任务中维持关于上下文的信念状态。

这里的贝叶斯可能很多人会懵逼，因为这主要是一种由因溯果的思维方式，下面举一个详细的例子：

### 贝叶斯理解的例子

想象一下你是一位非常聪明的侦探，需要解决一个案子。这里的“贝叶斯推断”就是你的破案思维方式。

我们先把文中的概念和侦探破案的角色对应起来：

* **上下文 (Context, C)**：这就是你的“嫌疑人”或“作案猜想”。世界上有无数种可能的上下文，就像有无数个潜在的嫌疑人一样。
* **用户的问题 (Query, c_query)**：这是你刚刚获得的“新线索”。
* **找到最佳上下文 (Find C\*)**：你的最终目标，也就是“锁定真凶”或“找到最合理的案情解释”。

传统的、非贝斯叶斯的方法可能像一个经验不足的警察：看到一个线索（用户问题），就直接去找表面上最相关的嫌疑人（上下文），比如问题里有“苹果”，就直接提供“苹果公司”的资料。

但贝叶斯侦探的思维方式更严谨、更周全。

---

### 第一步：评估所有“嫌疑人”的可能性（公式5）

> $P(C|c_{\text{query}}, \dots) \propto P(c_{\text{query}}|C) \cdot P(C|\text{History}, \text{World})$

这个公式是贝叶斯定理的核心，它告诉你如何根据新线索（`c_query`）来更新你对所有嫌疑人（`C`）的怀疑程度。我们把它拆开看：

#### 1. $P(C|\text{History}, \text{World})$ - **先验概率 (Prior)**
* **侦探的语言**：“在我看到这条新线索之前，根据案情卷宗（对话历史 `History`）和我的常识（`World`），我对每个嫌疑人的初始怀疑度是多少？”
* **实际意义**：在用户提问之前，系统会根据已经聊过的内容和通用知识，对哪些上下文可能相关有一个初步判断。比如，如果你们一直在聊物理学，那么关于“相对论”的上下文的“初始怀疑度”就很高，而关于“红楼梦”的上下文的“初始怀疑度”就很低。**这就是所谓的“先验知识”或“直觉”。**

#### 2. $P(c_{\text{query}}|C)$ - **似然 (Likelihood)**
* **侦探的语言**：“我做一个思想实验：**如果**张三是真凶（假设上下文C是正确的），那么他留下这条新线索（用户提出这个问题）的可能性有多大？”
* **实际意义**：这是在评估“上下文”和“问题”的匹配度。如果一个上下文C是关于“爱因斯坦生平”的，那么用户提出“相对论是谁发明的？”这个问题的可能性就非常高。但如果上下文C是关于“如何做蛋糕”，用户问“相对论是谁发明的？”的可能性就几乎为零。**这个步骤是在用线索来验证你的猜想。**

#### 3. $P(C|c_{\text{query}}, \dots)$ - **后验概率 (Posterior)**
* **侦探的语言**：“综合了我的‘初始怀疑度’和‘新线索的证据力’之后，我现在对每个嫌疑人的最终怀疑度是多少？”
* **实际意义**：这是我们最终想要的结果。它结合了先验（初始判断）和似然（证据匹配度），给每一个备选的上下文C打一个综合评分。评分最高的，就是当前最值得怀疑的“嫌疑人”。

**小结一下公式(5)在做什么**：它不是简单地找表面最相关的上下文，而是**将“初始的合理猜测”与“新证据的匹配程度”相乘，得出一个更可靠、更全面的判断**。

---

### 第二步：做出最终决策（公式6）

> $C^* = \arg\max_C \int P(Y|C, c_{\text{query}}) \cdot \text{Reward}(Y, Y^*) \,dY \cdot P(C|c_{\text{query}}, \dots)$

现在你已经对所有“嫌疑人”（上下文C）按怀疑度从高到低排了个序。但你还不能随便抓人。你需要考虑抓了之后的影响。

* **侦探的语言**：“锁定嫌疑最大的几个人之后，我需要思考一个问题：如果我把资源（注意力）放在嫌疑人A身上，最终能获得的破案成果（奖励）有多大？放在嫌疑人B身上呢？我要选择那个能让最终‘破案成果’最大化的嫌疑人。”

这个公式就是在做这个“决策”：

1.  它会考察那些得分高的上下文（`C`）。
2.  对于每一个 `C`，它会预测：“如果我把这个上下文 `C` 和用户的问题 `c_query` 一起给LLM，LLM可能会生成哪些答案 `Y`？（$P(Y|C, c_{\text{query}})$）”
3.  然后评估这些可能的答案质量如何？（$\text{Reward}(Y, Y^*)$）
4.  最后，它会选择那个**不仅自身可能性高，而且最有可能引导LLM产出高质量答案**的上下文 `C*`。

---

### 为什么这种贝叶斯方法更好？

1.  **处理不确定性**：它不假定有唯一正确的上下文，而是给所有可能性一个概率，然后优雅地处理这种不确定性。
2.  **自适应和学习**：随着对话的进行（`History`在变），它的“初始判断”（先验概率）会不断更新。这次对话的答案，会成为下次提问的“历史”，让系统越来越懂你。
3.  **更强的推理能力**：在需要多步骤才能解决的复杂问题上，这种方法可以一步步地更新自己的“信念状态”（对上下文的概率判断），从而实现更连贯、更深入的推理。

总而言之，这里的贝叶斯方法，就是让LLM的上下文选择过程从一个“凭感觉的匹配”，升级为一个“基于证据、不断更新信念、并以最终效果为导向”的严谨决策过程。

---
## 和提示词工程的区别

论文中给了一张上下文工程和提示词工程的图片，我觉得总结的很好。其实本质上来看，就是上下文工程更加庞大，更加复杂，更加偏向于庞大的文本信息管理工程。而提示词工程严格意义上来说是一种静态字符串的优化技术，更加偏向于各种trick的应用。**提示词工程是上下文工程的子集**。

![[Pasted image 20250721142033.png]]

---
# 基础组件

![[上下文工程的基础组件构成图.png]]

### 上下文检索和生成

### Prompt Engineering and Context Generation

> Prompt engineering and context generation forms the foundational layer of context retrieval, encompassing strategic input design that combines art and science to craft effective instructions for LLMs. The CLEAR Framework—conciseness, logic, explicitness, adaptability, and reflectiveness—governs effective prompt construction, while core architecture integrates task instructions, contextual information, input data, and output indicators.

主要涵盖了下面的一些技巧：

- Zero Shot 和 Few Shot 的使用，也就是利用了LLM的Context Learning的能力。
- 思维链基础
	- COT，最为简单的就是“一步一步思考”，这个技巧能够大大增加模型的性能。
	- TOT（思维树），将推理组织为搜索树，具有扩展和回溯的层次结构。
	- GOT（思维图），将不同的思维作为做节点，依赖关系作为边。（知识图谱类似）
- 认知架构集成
	- 认知提示实现类人的操作，包括目标澄清、分解、过滤、抽象和模式识别。
	- 其实就是将复杂的问题分解一个多步骤的解决算法，并且该算法是完备的，更加高级的问题还可以实现模块化推理，通过严谨的定义和结构化认知，模型的推理能力会在复杂问题上大大增加。（AIME MATH上从26%提升到了43%）

### External Knowledge Retrieval

这一个模块可以说是RAG里面最为核心的一部分，因为它解决的是模型的一个根本难题：**参数化知识难以动态更新**。

关于RAG框架的相关研究多如牛毛！下面挑选几个比较高级的经典框架讲讲：

- [Self-RAG](https://github.com/AkariAsai/self-rag)：其实就是给LLM赋予了一种反思的能力，具体表现在两个方面：
	- 按需检索：是否真的需要检索
	- 检查检索得到的内容：不是所有的内容都是完全符合回答这个问题的
	- 个人感觉非常类似于现在的DeepResearch用的那一个套路。
- [FlashRAG](https://github.com/RUC-NLPIR/FlashRAG)：对各种RAG模块和评估套件进行了高效的代码实现。
- [GraphRAG](https://github.com/microsoft/graphrag)：使用LLM生成知识图谱并且在知识图谱上进行知识检索。
- [KARPA](https://arxiv.org/abs/2412.20995)：通过预定义的规划，语义匹配和关系路径推理在知识图谱上的问答效果达到SOTA。（虽然这个论文我也还没读过）
- [Think On Graph](https://github.com/DataArcTech/ToG)：实现了知识图谱上的顺序推理以定位相关三元组，进行探索以从外部数据库检索相关信息，同时生成多个推理路径
- Agent将检索看成是一种动态的操作，一般来说Agent会将复杂问题进行规划和分解，Agent负责操作具体的检索逻辑和检索内容的引用。因此检索模块被设计成是即插即用的，支持不同的检索模块的组合使用，以解决Agent在执行任务的动态需求。

### Dynamic Contex Assemb

动态上下文组装代表了对获取的信息组件进行复杂编排，以形成连贯、任务优化的上下文，从而在尊重计算约束的同时最大化语言模型性能。在实际运用中，这块是难点中难点，能够参考的资料是很少的。

#### 组装函数和编排机制

组装函数包括**模板的格式化、基于优先级的选中和自适应策略**。而函数又必须要考虑到计算的约束，模型能力和不同的任务需求。当前的现代高级编排机制通常包含下面的内容：
- Agent的选择
- 上下文的分布
- 交互流设计
- 意图识别
- 上下文持久化的记忆
- 自适应任务分配

#### 多组件集成的策略

上下文的组装必须要解决跨模态，跨数据格式的数据集成挑战。当前可以使用LLM本身来处理将不同的数据格式数据序列化为自然语言。而反向的，为了使用关系推理（SQL或者知识图谱），必须要依赖于LLM来反序列化自然语言为结构化的数据从而进行更加高效精准的结构化知识检索。

#### 自动化组装优化

自动化提示工程通过系统化提示生成和优化算法解决手动设置的限制。（这个目前在Agent方面用的比较多，比如Self-Refine的机制，Role-Play类型的Agent）

---
# 上下文处理

上下文处理专注于转换和优化获取的上下文信息，以最大化其对大型语言模型（LLM）的效用。主要包含三个方面：长上下文的处理，支持自我完善和迭代的机制，将多模态信息、关系和结构化信息整合为连贯的上下文表示。

## 模型注意力改进

SSM适用于长上下文，因为其保持固定大小的隐藏状态，所以能够保持恒定的内存需求，而Mamba这种类型的模型提供了高效的循环计算机制（找时间在拜读一下），扩展效果比传荣的Transformer要好。而有一些研究则直接扩展了Transformer的注意力，比如LongNet利用指数扩展的注意力字段，实现线性的计算复杂度同时保持token之间的对数依赖关系，能够处理超过十亿个token。（代价是时间换空间，或者是略微的性能损失换取高额长上下文）

除此之外还有非常多的方法能够优化Transformer的注意力，意在打破其中的$O(n^2)$的计算复杂度，可惜目前大多数方法没有在大规模模型上得到验证。

## 位置插值和上下文扩展

位置插值技术使模型能够通过智能地重新缩放位置索引来处理超出原始上下文窗口限制的序列，而不是推断到未见过的位置，这个首先提出来的论文是ROPE旋转位置编码。由于这个技巧非常实用并且代价极小，是目前的模型的主流做法。

## 高效处理的优化技术

除了打破模型架构之外，还有一些比较优秀的优化方法，比如针对传统Transformer的MHA做改变的GQA, MLA，还有针对内存优化的算法Flash-Attention，除此之外，还有一些稀疏注意力、分块注意力等等方法，这些方法均使用了很小的性能损失（或者无损）换取了很高的长上下文处理能力。

## 内存管理和上下文压缩

一个比较naive的做法就是维护一个固定大小的缓冲区（这是主流的做法），在此基础上，StreamingLLM保留了关键的一些token及其对应的kv cache，从而可以实现无限长的序列处理。Infini‑attention将压缩内存整合到普通的注意力机制中，结合掩码局部注意力和长期线性注意力在单个Transformer块中，实现了在有限的内存和计算量下对无限长输入的处理 。
Oracle(H2O)基于观察到小部分token贡献了大部分注意力值的观察结果，提出了高效的KV缓存驱逐策略，通过最多 29× 提升了吞吐量，同时将延迟降低了最多1.9× 。上下文压缩技术如QwenLong‑CPRS实现了动态上下文优化机制，**通过自然语言指令指导多粒度压缩**。

## 上下文精炼、优化和适应

一般来说，要提供一个有效的上下文改善机制，基于这个机制和反馈，LLM能够自我评估从而优化上下文。这通常来源于一个关键的事实：不断迭代改进比直接生成一个完美的方案要简单很多。


