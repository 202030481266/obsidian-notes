
# Paper URL

[Accessing GPT-4 level Mathematical Olympiad Solutions via Monte Carlo Tree Self-refine with LLaMa-3 8B](http://arxiv.org/abs/2406.07394)

# Reading Reflection

## 相关的一些链接

- 量子位文章：[8B模型奥数成绩比肩GPT-4！上海AI Lab出品 (qq.com)](https://mp.weixin.qq.com/s/vrmkeTeU92QsZN8DPJqj7w)
    
- 官方代码仓库：[https://github.com/trotsky1997/mathblackbox](https://github.com/trotsky1997/mathblackbox)
    

## 蒙特卡洛搜索树(MCTS)

参考文章进行学习：[面向初学者的蒙特卡洛树搜索MCTS详解及其实现_mcts算法-CSDN博客](https://blog.csdn.net/caozixuan98724/article/details/103213795)

参考实现代码：[pbsinclair42/MCTS: A simple package to allow users to run Monte Carlo Tree Search on any perfect information domain (github.com)](https://github.com/pbsinclair42/MCTS)

总结来说，其实就是下面的几个步骤：

蒙特卡洛树搜索（Monte Carlo Tree Search，简称MCTS）是一种用于决策过程的算法，特别常用于游戏AI中，例如围棋和国际象棋。它结合了随机模拟和树搜索的优点。下面是对其工作原理的简要介绍，适合初学者理解。

### 1. 基本概念

- **树**：在计算机科学中，树是一种数据结构，由节点（node）组成。树的根节点是开始的地方，每个节点可以有零个或多个子节点。
    
- **蒙特卡洛方法**：一种利用随机采样进行数值计算的方法。简单来说，就是通过大量随机模拟来逼近问题的解。
    
### 2. MCTS的四个步骤

MCTS的核心思想是通过模拟来评估每一个可能的行动，逐渐建立起一个决策树。其主要步骤包括：

#### 1. 选择（Selection）

从根节点开始，根据某种策略（例如UCB1，上置信界）选择一个子节点，沿着这条路径一直往下走，直到到达一个尚未完全展开（有潜在子节点但未被探索）的节点。**一般来说都会使用UCT算法来权衡“经验”和“探索”。探索意味着Expand新的节点，经验则说明走已有的节点。**

#### 2. 扩展（Expansion）

从选择的节点扩展一个或多个子节点。通常，只扩展一个未被访问过的节点。

#### 3. 模拟（Simulation）

从新扩展的节点开始，进行一次随机模拟（也称为playout），直到达到终局。这一步的目的是通过随机模拟来评估当前节点的价值。

#### 4. 回溯（Backpropagation）

将模拟的结果反向传播到树中的每个节点，更新这些节点的统计信息（例如获胜次数、访问次数）。这样，树中每个节点都会包含到达这个节点的所有路径的平均结果。

### 3. 优势

- **有效性**：MCTS可以在有限的时间内给出一个较好的决策，不需要预先知识。
    
- **灵活性**：适用于各种不同类型的问题，特别是在状态空间非常大的情况下。
    
- **逐步改进**：随着时间的推移，MCTS会越来越好地逼近最优解。
    

具体的算法可以详细看代码，很容易理解。

## MCTS和LLM的结合

具体来讲还是MCTS的那一套方法，不过每一个步骤都修改为对LLM的适应，Self-Refine方法可以缓解LLM应对数学推理产生的幻觉，而本文就是结合了两种方法：MCT+Self Refine(MCTSr)。

下面一个一个讲讲比较难和创新的地方。

### 自我评估的过程（相当于是MCTS的Simulation阶段）

1. **初始化解答**：模型会先生成一个初始解答，这个解答可能比较简单或者直接是“我不知道”。
    
2. **解答优化**：通过自我反思和反馈，模型会尝试改进这个解答，生成一个更好的版本。
    
3. **奖励打分**：**模型会对改进后的解答进行打分**，这个分数范围在-100到100之间。为了确保打分的严谨性，设计了三个约束：（这算是一个创新点了）
    
    - **提示约束**：模型在打分时需要遵循最严格的标准。
        
    - **满分抑制**：模型被要求不要轻易给出满分，任何高于95分的评分都会被减去一个固定值。
        
    - **重复采样**：每次访问一个节点时，都会多次采样其奖励值，以提高评分的可靠性。
        
4. **计算Q值**：Q值表示解答的质量，是通过多个采样的奖励值来计算的。公式如下：
    
    $Q(a)=\frac{1}{2}\left(\min(R_a)+\frac{1}{|R_a|}\sum_{i=1}^{|R_a|}R_{a,i}\right)$
    
    这里 $\min(R_a)$ 是所有采样奖励值中的最小值， $\sum_{i=1}^{|R_a|} R_{a,i}$ 是所有采样奖励值的总和，$|R_a|$ 是采样的次数（即奖励值的数量）。
    
### MCTSr反向传播（相当于是MCTS的Backpropagation阶段）

反向传播在MCT Self-Refine算法中用于更新节点的质量值（Q值），从而反映新评估结果对整个搜索树的影响。这个过程确保了每次改进解答后的反馈能够正确传递到父节点及其他相关节点，提升整体决策质量。具体步骤如下：

1. **叶节点奖励值更新**：在对所有叶节点的奖励值进行采样并更新其Q值后，开始进行反向传播。

2. **父节点Q值更新**：

- 如果某个节点的子节点的Q值发生变化，那么该节点的Q值需要更新。
    
- 更新公式为：
    
    $Q'(a) = \frac{1}{2} \left( Q(a) + \max_{i \in \text{Children}(a)} Q(i) \right)$
    
- 其中：
    
    - $Q'(a)$是节点 $a$ 的更新后的Q值。
        
    - $Q(a)$ 是节点 $a$ 的当前Q值。
        
    - $\max_{i \in \text{Children}(a)} Q(i)$ 是节点 $a$ 的子节点中最大的Q值。
        

3. **递归更新**：

- 这个过程会递归地从叶节点向上更新，直到根节点。每次更新父节点的Q值时，都需要检查其所有子节点的Q值，以确保反映最新的评估结果。
    
### 马尔科夫性质（Markov Property，感觉是一个大的创新）

#### 定义

马尔科夫性质是指一个系统的未来状态仅依赖于其当前状态，而与过去的状态无关。通俗地说，就是“记忆无关性”，即当前所处的状态已经包含了过去所有信息，未来的发展只依赖于当前状态。

简单来讲就是MCTS的节点被认为是包含了过去状态所有的信息的完全体，并不依赖于任何一个前面的状态，这说明路径和状态不存在依赖关系。

#### 应用在算法中的含义与作用（提升效率）

在MCT Self-Refine算法中的马尔科夫性质意味着：

1. **当前状态独立**：**每个节点（解答）的当前质量（Q值）只依赖于最近一次的改进和当前的评估结果**，而不需要考虑之前所有的改进路径。
    
2. **路径独立性**：当选择候选节点进行进一步改进时，**只需考虑当前所有的叶节点和未完全展开的节点，而无需考虑这些节点是如何到达当前位置的**。
    

假设我们有一个问题需要多次改进来找到最佳解答。在传统方法中，可能需要追踪每次改进的路径，记录所有步骤（Actions）：

- 初始解答 -> 第一次改进 -> 第二次改进 -> ...
    

在具有马尔科夫性质的情况下，只需记录当前解答的状态，不用关心它是通过哪些步骤达到当前状态的：

- 当前解答状态（已包含所有信息）
    

因此，**在选择候选节点时，可以直接从当前所有的叶节点和未完全展开的节点中选择，而不需要回溯到根节点（搜索变成了集合维护）**。这不仅减少了计算量，还提高了选择的效率。

### 贝叶斯优化期望改进算法

贝叶斯优化是一种用于全局优化问题的策略，特别适用于昂贵的目标函数。期望改进（EI）是贝叶斯优化中的一种获取函数（acquisition function），它在每一步选择最有希望改进当前最优解的点。

#### 期望改进的公式（实际上这也是MCTS的Selection阶段的数学原理）

期望改进的公式通常表示为： $EI(x) = E\left[ \max(0, f(x) - f(x^*)) \right]$ 其中：

- $f(x)$是在点$x$处的目标函数值。
    
- $f(x^*)$是当前最优点 $x^*$处的目标函数值。
    
- $E[\cdot]$表示期望值。
    

EI 的计算基于目标函数的预测分布（通常由高斯过程模型提供），它结合了探索和利用，通过计算每个候选点的潜在改进来选择下一个评估点。

基于贝叶斯优化的期望改进，论文提出了两个标准判断一个节点是否完全展开（**实际上就是启发式搜索**）：

- 超过了预定数量的ChildNodes。（探索到达极限，完全展开）
    
- 至少一个ChildNode的Q值大于该节点。（存在探索潜力，未完全展开）
    
### UCT算法选择节点（MCTS的Selection阶段）

UCT选择方法在MCT Self-Refine算法中用于平衡探索和利用，通过选择潜在改进最大的节点进行进一步探索。UCT方法源自蒙特卡罗树搜索（MCTS），其核心在于通过一个带有不确定性（confidence bound）的价值函数来选择节点。

### 公式与计算

UCT选择方法使用以下公式来计算每个候选节点的UCT值（**_应该是收到了AlphaGo的启发，这个函数效果很好_**）：$UCT(a) = Q(a) + c \sqrt{\frac{\ln(N(\text{Father}(a)) + 1)}{N(a) + \epsilon}}$

其中：

- $Q(a)$是节点 $a$ 的质量值。
    
- $N(\text{Father}(a))$是节点 $a$的父节点的访问次数。
    
- $N(a)$ 是节点 $a$ 的访问次数。
    
- $c$ 是一个常数，用于平衡探索与利用。
    
- ϵ 是一个小常数，用于避免分母为零的情况。
    
### 选择过程

1. **候选节点的确定**：
    
    - **全展开判断**：确定哪些节点已经完全展开（根据贝叶斯优化的期望改进）。
        
    - **候选节点集合**：选择所有叶节点和未完全展开的节点作为候选节点。
        
2. **UCT值计算**：
    
    - 对于每个候选节点 a，计算其UCT值。
        
3. **节点选择**：
    
    - 根据计算出的UCT值，对候选节点进行排序。
        
    - 选择UCT值最高的节点进行进一步的探索和改进。
        
### Terminal Function介绍

1. **Early Stopping（早停法）**：
    
    - **定义**：当搜索结果的改进幅度减小时，或者连续多次搜索得到相同或相似的结果时，算法提前停止搜索。
        
    - **原因**：避免在改进效果不显著时浪费计算资源，及时结束搜索过程。
        
    - **作用**：提高算法效率，防止过度计算。
        
2. **Search Constraints（搜索限制）**：
    
    - **定义**：当搜索次数达到预设的上限，或某个节点达到最大搜索深度时，终止搜索。
        
    - **原因**：通过设置搜索次数或深度的上限，控制算法的复杂度和运行时间。
        
    - **作用**：防止搜索过程过长，确保算法在有限时间内完成。
        
3. **Advanced Criteria Based on Language Model Logits（基于语言模型logits的高级标准）**：
    
    - **定义**：基于语言模型logits的预定义度量标准来决定搜索终止。
        
    - **原因**：利用语言模型的内在评估机制，确保搜索结果达到一定的质量标准。
        
    - **作用**：提高搜索结果的质量，确保终止时得到的解答具有高可靠性和准确性。
        

第三点还挺难理解的，说白了其实就是perplextiy那一套，**Logits代表了模型的输出概率分布，也表示了语言模型的语言分布或者相对于某一个答案的困惑度**。

1. **置信度阈值**：如果生成结果的logits超过某个置信度阈值，表示模型对当前解答非常有信心，可以终止搜索。
    
2. **变化率阈值**：如果最近几次生成结果的logits变化幅度很小，表示模型的生成结果趋于稳定，可以终止搜索。
    
3. **累积置信度**：根据一段时间内生成结果的平均logits，判断整体搜索质量，决定是否终止。