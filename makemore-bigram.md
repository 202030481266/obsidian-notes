# Related Links
[Karpaphy's jupyter lab code](https://github.com/karpathy/nn-zero-to-hero/blob/master/lectures/makemore/makemore_part1_bigrams.ipynb)
[Karpaphy's lecture on Youtube](https://www.youtube.com/watch?v=PaCmpygFfXo)
[makemore/names.txt at master Â· karpathy/makemore (github.com)](https://github.com/karpathy/makemore/blob/master/names.txt)
[Shulin's colab notebook](https://drive.google.com/drive/folders/1dlmTg-rbVvrxN2dc3eUDIvCs_Cns9DPB)
# Jupyter Lab Code

æˆ‘ä»¬åœ¨Google Colabå®Œå…¨å¤ç°è¿™ä¸ªäºŒå…ƒæ¨¡å‹ï¼ˆå­—ç¬¦çº§åˆ«çš„äºŒå…ƒè¯­è¨€æ¨¡å‹ï¼Œé¢„æµ‹æ¯ä¸€ä¸ªå­—ç¬¦çš„æ—¶å€™éƒ½åªä¼šå…³æ³¨å‰ä¸€ä¸ªå­—ç¬¦æ˜¯ä»€ä¹ˆï¼‰ã€‚

```python
import torch
import requests

# ä¸‹è½½æ•°æ®é›†
url = "https://raw.githubusercontent.com/karpathy/makemore/master/names.txt"
response = requests.get(url)
if response.status_code == 200:
Â  Â  with open("names.txt", "w") as file:
Â  Â  Â  Â  file.write(response.text)Â  
Â  Â  print("File 'names.txt' has been downloaded successfully.")
else:
Â  Â  print(f"Failed to download the file. Status code: {response.status_code}") 
```

ç»Ÿè®¡ä¿¡æ¯å‡½æ•°ï¼ŒæŸ¥çœ‹å…¶ä¸­çš„ä¸€äº›äºŒå…ƒç»„ä¿¡æ¯ã€‚ï¼ˆå¾—åˆ°ä¸€ä¸ªåˆæ­¥çš„æ„ŸçŸ¥ï¼‰

```python
def describe_dataset(words):
  # æ˜¾ç¤ºæ•°æ®é›†çš„ç»Ÿè®¡ä¿¡æ¯
  b = {}
  for w in words:
    chs = ['<S>'] + list(w) + ['<E>'] # æ·»åŠ èµ·å§‹å’Œç»ˆæ­¢çš„token
    for c1, c2 in zip(chs, chs[1:]):
      bigram = (c1, c2)
      b[bigram] = b.get(bigram, 0) + 1
  mx = sorted(b.items(), key = lambda kv : -kv[1]) # æŒ‰ç…§å‡ºç°çš„æ¬¡æ•°ä»å¤§åˆ°å°æ’åº
  print(mx[:10])
```

å¯¹äºæ¯ä¸€ä¸ªè‹±æ–‡å­—ç¬¦ä½¿ç”¨ä¸€ä¸ªç´¢å¼•ä½œä¸ºç¼–ç ï¼Œç„¶åä½¿ç”¨`.`ä½œä¸ºspecial tokenï¼Œä¹Ÿå°±æ˜¯å¼€å§‹çš„ç‚¹å’Œç»“æŸçš„ç‚¹ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥è¿›è¡Œä¸€ä¸ªç»Ÿè®¡å¾—åˆ°æ‰€æœ‰çš„äºŒå…ƒå­—ç¬¦ç»„æ•°é‡ï¼Œå¹¶ä¸”åšä¸€ä¸ªå¯è§†åŒ–ã€‚

```python
# ç¬¬ä¸€ç§è®­ç»ƒæ¨¡å‹çš„æ–¹æ³•ï¼Œç›´æ¥ä»æ•°æ®é›†ä¸­è¿›è¡Œç»Ÿè®¡ï¼ˆç»Ÿè®¡é¢‘ç‡ï¼‰

import matplotlib.pyplot as plt
%matplotlib inline

def count_matrix(words):
  N = torch.zeros((27, 27), dtype=torch.int32) # ç»Ÿè®¡é¢‘ç‡è¡¨
  chars = list(set(''.join(words))) # å»é‡
  # å®é™…ä¸Šï¼Œæˆ‘ä»¬æ­£åœ¨æ„å»º tokenizer
  stoi = {ch : i + 1 for i, ch in enumerate(chars)}
  itos = {i + 1 : ch for i, ch in enumerate(chars)}
  stoi['.'] = 0 # special token
  itos[0] = '.'

  # è®¡æ•°
  for w in words:
    chs = ['.'] + list(w) + ['.']
    for ch1, ch2 in zip(chs, chs[1:]):
      ix1 = stoi[ch1]
      ix2 = stoi[ch2]
      N[ix1, ix2] += 1
  
  # å¯è§†åŒ–
  plt.figure(figsize=(16, 16))
  plt.imshow(N, cmap='Blues')

  for i in range(27):
    for j in range(27):
      chstr = itos[i] + itos[j]
      plt.text(j, i, chstr, ha='center', va='bottom', color='gray')
      plt.text(j, i, N[i, j].item(), ha='center', va='top', color='gray')
  plt.axis('off')
  plt.show()
  return N
```

ä¸‹é¢å°±æ˜¯è®­ç»ƒæ¨¡å‹çš„ä¸€èˆ¬æ­¥éª¤ï¼Œæœ‰å‡ ä¸ªå…³é”®çš„ç‚¹ï¼š
1. `torch.Generator().manual_seed()` ç”¨æ¥æ‰‹åŠ¨è®¾ç½®éšæœºç”Ÿæˆæ•°çš„ç§å­ï¼Œå¯ä»¥ä¿è¯æ•°å­—çš„ä¸€è‡´æ€§ï¼Œä¸€èˆ¬ç”¨æ¥æµ‹è¯•ä¸åŒçš„ä»£ç æ˜¯å¦å…·æœ‰ç›¸åŒçš„è¾“å…¥å’Œè¾“å‡ºã€‚
2. `sum()` å‡½æ•°çš„è®¡ç®—å±äºèšåˆè¿ç®—ï¼Œå¹¶ä¸”è¦ä¿æŒç»´åº¦ï¼Œä¸ç„¶åé¢å› ä¸ºå¹¿æ’­æœºåˆ¶ä¼šå¯¼è‡´é”™è¯¯ã€‚
3. `torch.multinomial()` ç”¨æ¥æ ¹æ®ç»™å®šçš„å‘é‡ç”Ÿæˆç¬¦åˆå…¶ä¸­æ¦‚ç‡åˆ†å¸ƒçš„ç´¢å¼•ï¼ˆå‘é‡ä¹‹å’Œä¸ä¸€å®šç­‰äº1ï¼‰ï¼Œæ‰§è¡Œæ¬¡æ•°è¶Šå¤šåˆ†å¸ƒå°±è¶Šç¬¦åˆã€‚([details for multinomial](https://pytorch.org/docs/stable/generated/torch.multinomial.html))

```python
# normalize
p = (N+1).float()  # è®©æ¨¡å‹æ›´åŠ smoothï¼ŒåŠ 1æ˜¯ä¸ºäº†é˜²æ­¢çˆ†0
p /= p.sum(1, keepdim=True) # keepdimå¿…é¡»æ˜¯Trueï¼Œä¸ç„¶å°±æ˜¯å¹¿æ’­æœºåˆ¶
g = torch.Generator().manual_seed(2147483647)  # æ‰‹åŠ¨è®¾ç½®éšæœºæ•°ç§å­
for i in range(5):
  out = []
  ix = 0
  while True:
    c = p[ix]
    # c = torch.rand((1, 27)) å¦‚æœæƒ³çœ‹å¯¹æ¯”çš„æ¦‚ç‡åˆ†å¸ƒ
    ix = torch.multinomial(c, num_samples=1, replacement=True, generator=g).item()
    out.append(itos[ix])
    if ix == 0:
      break
  print(''.join(out))
```

Karpathyä¹Ÿæåˆ°äº†è¿™ä¸ªäºŒå…ƒæ¨¡å‹å®åœ¨æ˜¯åƒåœ¾åˆ°éš¾ä»¥ç½®ä¿¡ï¼Œä½†æ˜¯æ€»æ˜¯æ¯”éšæœºçš„æ¦‚ç‡åˆ†å¸ƒäº§ç”Ÿçš„å•è¯å¥½å¾—å¤šï¼

ä¸‹é¢æ˜¯åŸºäºç¥ç»ç½‘ç»œ+åå‘ä¼ æ’­ç®—æ³•å®ç°çš„æ¨¡å‹ã€‚é¦–å…ˆä»‹ç»æœ€å¤§ä¼¼ç„¶å‡½æ•°çš„ä¸€äº›å…³é”®æ¦‚å¿µï¼ˆå‡ ç™¾å¹´å‰å­¦è¿‡äº†ï¼Œå¿˜å¾—éƒ½å·®ä¸å¤šäº†ï¼‰ã€‚

å¯¹æ•°ä¼¼ç„¶å‡½æ•°çš„æ•°å­¦åŸç†åŸºäºæ¦‚ç‡è®ºå’Œç»Ÿè®¡å­¦ä¸­çš„ä¼¼ç„¶åŸç†ã€‚å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªæ•°æ®é›† $$\mathbf{X} = \{x_1, x_2, \ldots, x_n\}$$ï¼Œå¹¶ä¸”æˆ‘ä»¬æœ‰ä¸€ä¸ªå‚æ•°åŒ–çš„æ¦‚ç‡æ¨¡å‹ $p(x \mid \theta)$ï¼Œå…¶ä¸­ $\theta$ æ˜¯æ¨¡å‹å‚æ•°ã€‚

**ä¼¼ç„¶å‡½æ•°** $L(\theta \mid \mathbf{X})$ å®šä¹‰ä¸ºç»™å®šæ•°æ®é›† $\mathbf{X}$ ä¸‹ï¼Œå‚æ•° $\theta$ çš„æ¦‚ç‡ï¼š

$$ L(\theta \mid \mathbf{X}) = \prod_{i=1}^n p(x_i \mid \theta) $$

è¿™é‡Œï¼Œä¼¼ç„¶å‡½æ•°æ˜¯æ‰€æœ‰å•ä¸ªæ•°æ®ç‚¹æ¦‚ç‡çš„ä¹˜ç§¯ï¼Œå› ä¸ºå‡è®¾æ¯ä¸ªæ•°æ®ç‚¹æ˜¯ç‹¬ç«‹åŒåˆ†å¸ƒçš„ã€‚

**å¯¹æ•°ä¼¼ç„¶å‡½æ•°** $\log L(\theta \mid \mathbf{X})$ æ˜¯å¯¹ä¼¼ç„¶å‡½æ•°å–è‡ªç„¶å¯¹æ•°ï¼š

$$ \log L(\theta \mid \mathbf{X}) = \sum_{i=1}^n \log p(x_i \mid \theta) $$

å–å¯¹æ•°çš„å¥½å¤„æ˜¯å°†ä¹˜æ³•è½¬æ¢ä¸ºåŠ æ³•ï¼Œè¿™åœ¨è®¡ç®—ä¸Šæ›´ç¨³å®šï¼Œä¹Ÿæ›´å®¹æ˜“å¤„ç†ã€‚

**æœ€å¤§å¯¹æ•°ä¼¼ç„¶ä¼°è®¡**ï¼ˆMaximum Log-Likelihood Estimation, MLEï¼‰æ˜¯æ‰¾åˆ°ä½¿å¾—å¯¹æ•°ä¼¼ç„¶å‡½æ•°æœ€å¤§çš„å‚æ•° $\theta$ï¼š

$$ \hat{\theta}_{\text{MLE}} = \arg \max_{\theta} \log L(\theta \mid \mathbf{X}) $$

åœ¨å®é™…åº”ç”¨ä¸­ï¼Œæˆ‘ä»¬é€šå¸¸é€šè¿‡æ±‚è§£å¯¹æ•°ä¼¼ç„¶å‡½æ•°çš„æ¢¯åº¦ç­‰äºé›¶çš„æ–¹ç¨‹æ¥æ‰¾åˆ°è¿™ä¸ªæœ€å¤§å€¼ï¼š

$$ \frac{\partial \log L(\theta \mid \mathbf{X})}{\partial \theta} = 0 $$

è¿™ä¸ªæ–¹ç¨‹é€šå¸¸æ˜¯éçº¿æ€§çš„ï¼Œå¯èƒ½éœ€è¦ä½¿ç”¨æ•°å€¼æ–¹æ³•æ¥æ±‚è§£ã€‚åœ¨æ·±åº¦å­¦ä¹ ä¸­ï¼Œé€šå¸¸å°†è¿™ä¸ªå¯¹æ•°ä¼¼ç„¶ä½œä¸ºä¸€ä¸ªæŸå¤±å‡½æ•°ï¼Œä½†æ˜¯æ·±åº¦å­¦ä¹ ä¸­ä¸€èˆ¬æ˜¯**æœ€å°åŒ–æŸå¤±**ï¼Œæ‰€ä»¥ä¸Šé¢çš„æœ€å¤§åŒ–å¯¹æ•°ä¼¼ç„¶ç­‰ä»·äºæœ€å°åŒ–**è´Ÿå¯¹æ•°ä¼¼ç„¶å‡½æ•°** $-\log L(\theta \mid \mathbf{X})$ ï¼Œå…¶å®å°±æ˜¯å¯¹æ•°ä¼¼ç„¶å‡½æ•°çš„è´Ÿå€¼ï¼š

$$-\log L(\theta \mid \mathbf{X}) = -\sum_{i=1}^n \log p(x_i \mid \theta)$$

$$\hat{\theta}_{\text{MLE}} = \arg \min_{\theta} -\log L(\theta \mid \mathbf{X}) $$
ç®€å•è®¡ç®—ä¸€ä¸‹æ•°æ®é›†ä¸­çš„æ•´ä½“çš„è´Ÿæ•°å¯¹æ•°ä¼¼ç„¶æŸå¤±ï¼š

```python
log_likelihood = 0.0
n = 0

for w in words:
  chs = ['.'] + list(w) + ['.']
  for ch1, ch2 in zip(chs, chs[1:]):
    ix1 = stoi[ch1]
    ix2 = stoi[ch2]
    prob = p[ix1, ix2]
    logprob = torch.log(prob)
    log_likelihood += logprob
    n += 1

print(f'{log_likelihood=}')
nll = -log_likelihood
print(f'{nll=}')
print(f'{nll/n}')
```

è®¾è®¡ç¥ç»ç½‘ç»œçš„æ¶æ„ï¼Œç”±äºæˆ‘ä»¬çš„è¾“å…¥æ˜¯äºŒå…ƒç»„$(x_i,y_i)$è¿™ç§å½¢å¼ï¼Œä¸èƒ½ç›´æ¥åº”ç”¨åˆ°ç¥ç»ç½‘ç»œã€‚ä¸€ä¸ªå¸¸è§çš„æ‰‹æ®µæ˜¯ä½¿ç”¨`one-hot`ç¼–ç ã€‚å¯¹äºæ¯ä¸€ä¸ª$x_i$ï¼Œæˆ‘ä»¬å°†å…¶ç¼–ç æˆä¸º

$$x=[0,0,0,1,0,0,...,0]$$
è¿™ç§å½¢å¼çš„å‘é‡ã€‚æˆ‘ä»¬å‡è®¾æ•°æ®é›†çš„æ‰¹æ¬¡æ˜¯$B$ï¼Œé‚£ä¹ˆè¾“å…¥å°±æ˜¯$B\times27$çš„çŸ©é˜µã€‚åŒæ ·çš„æˆ‘ä»¬è¾“å‡ºä¹Ÿåº”è¯¥æ˜¯è¿™ä¸ªå¤§å°çš„çŸ©é˜µã€‚Karpathyå¤§ç¥ç”¨äº†æœ€ç®€å•çš„çº¿æ€§å›å½’æ–¹æ³•ï¼Œå°±æ˜¯ä¸€å±‚ç½‘ç»œçš„æ„ŸçŸ¥æœºæ¨¡å‹ï¼š

$$Y=XW$$
å…¶ä¸­$Y,X\in\mathbb{R}^{B\times27}$ï¼Œè€Œå…¶ä¸­çš„$W\in \mathbb{R}^{27\times27}$ ã€‚æ€è€ƒä¸€ä¸‹å‚æ•°çŸ©é˜µé‡Œé¢çš„æ¯ä¸€ä¸ªå…ƒç´ åˆ°åº•æ˜¯ä»€ä¹ˆæ„æ€å‘¢ï¼Ÿå®é™…ä¸Šï¼Œå‚æ•°çŸ©é˜µååº”çš„æ„ä¹‰å°±æ˜¯$N$ ï¼Œä»£ç ä¸­çš„é‚£ä¸ªè®¡æ•°ç»Ÿè®¡è¡¨ã€‚ï¼ˆå¯ä»¥åŠ¨æ‰‹ç®—ç®—ï¼‰

```python
# æ„é€ æ•°æ®é›†å¹¶ä¸”è¿›è¡Œè®­ç»ƒ
import torch.nn.functional as F

xs, ys = [], []
for w in words:
  chs = ['.'] + list(w) + ['.']
  for ch1, ch2 in zip(chs, chs[1:]):
    ix1 = stoi[ch1]
    ix2 = stoi[ch2]
    xs.append(ix1)
    ys.append(ix2)
xs = torch.tensor(xs)
ys = torch.tensor(ys)
num = xs.nelement()
print(f'the number of this dataset is {num}')

g = torch.Generator().manual_seed(2147483647)
W = torch.rand((27, 27), generator=g, requires_grad=True) # åˆ«å¿˜äº†åŠ ä¸Šæ¢¯åº¦

for k in range(200):
  
  # forward pass
  xenc = F.one_hot(xs, num_classes=27).float() # one-hot encode
  logits = xenc @ W # predict, [num, 27]
  counts = logits.exp() # soft-max [num, 27]
  probs = counts / counts.sum(1, keepdim=True) # normalize
  loss = -probs[torch.arange(num), ys].log().mean() + 0.01 * (W**2).mean()
  if k % 10 == 0:
    print(f'{loss.item()}')

  # backward pass
  W.grad = None  # æ¢¯åº¦æ¸…é›¶
  loss.backward()

  # update
  W.data += -10 * W.grad
```

ä»£ç é‡Œé¢å…¶å®è¿˜åŒ…å«äº†ä¸€ä¸ªæ­£åˆ™åŒ–æŸå¤±`W**2` ï¼Œè¿™ä¸ªæ„æ€å°±æ˜¯æœ€åå¾—åˆ°çš„å‚æ•°ä¼šæ›´åŠ å‡åŒ€ï¼Œå¹¶ä¸”è®©æ•°å­—è¶‹å‘äº0ï¼Œè¿™å°±èƒ½å¤Ÿè®©è®­ç»ƒæ›´åŠ ç¨³å®šã€‚

æœ€åï¼Œçœ‹çœ‹æˆ‘ä»¬çš„æ¨¡å‹æˆæœï¼ğŸ”¥ğŸ”¥ğŸ”¥

```python
for i in range(5):
  
  out = []
  ix = 0
  while True:
    
    # ----------
    # BEFORE:
    #p = P[ix]
    # ----------
    # NOW:
    xenc = F.one_hot(torch.tensor([ix]), num_classes=27).float()
    logits = xenc @ W # predict log-counts
    counts = logits.exp() # counts, equivalent to N
    p = counts / counts.sum(1, keepdims=True) # probabilities for next character
    # ----------
    
    ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()
    out.append(itos[ix])
    if ix == 0:
      break
  print(''.join(out))
```

