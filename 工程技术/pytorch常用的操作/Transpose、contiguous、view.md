# 官方文档

[torch.transpose](https://pytorch.org/docs/stable/generated/torch.transpose.html#torch.transpose)
[torch.Tensor.contiguous](https://pytorch.org/docs/stable/generated/torch.Tensor.contiguous.html)
[torch.Tensor.view](https://pytorch.org/docs/stable/generated/torch.Tensor.view.html)
# From a Question

要理解这三个函数，还要从实现transformer遇到的问题说起：

```python
# [bsz, n_qh, s, cache_len + seq_len]
scores = F.softmax(scores, dim=-1)
# [bsz, n_qh, s, cache_len + seq_len] * [bsz, n_qh, cache_len + seq_len, d_h] -> [bsz, n_qh, s, d_h]
output = torch.matmul(scores, values)
# [bsz, n_qh, s, d_h] -> [bsz, s, n_qh, d_h] -> [bsz, s, d]
output = output.transpose(1, 2).contiguous().view(bsz, seqlen, -1)
```

在最后的输出中，`output`变量使用了一个`contiguous`函数来创建连续的张量，直接`view`不行吗？这里面就牵扯到了一些底层的细节。`view`函数针对的张量在内存中的布局必须是连续的，这里简单说一下。
## 什么是内存连续？

在 PyTorch 中，一个张量是内存连续的，意味着它的元素在内存中是按照某种顺序（通常是行优先，row-major）连续存储的，没有“跳跃”或“间隔”。连续性直接影响 `view` 等操作是否能成功，因为这些操作依赖于数据的线性排列。

步长（stride）描述了从一个元素到“逻辑上相邻元素”在内存中需要跳过的元素数量。如果步长与张量的形状匹配预期连续存储的模式，这个张量就是连续的。
## 如何通过步长判断连续性？

判断张量是否连续，核心是检查它的步长是否满足“连续存储”的规则。连续存储的步长规则是：
- 对于一个多维张量，假设它的形状是 `(d_0, d_1, ..., d_n)`（从最外层维度到最内层），理论上的连续步长应该是：
  - 最内层维度（`d_n`）的步长是 `1`。
  - 倒数第二层维度（`d_{n-1}`）的步长是 `d_n`。
  - 倒数第三层维度（`d_{n-2}`）的步长是 `d_{n-1} * d_n`。
  - 以此类推，最外层维度（`d_0`）的步长是 `d_1 * d_2 * ... * d_n`。

换句话说，步长是从最内层到最外层累积乘积的结果。

#### 判断步骤：
1. **获取张量的形状和步长**：
   - 形状可以用 `.shape` 或 `.size()` 查看。
   - 步长可以用 `.stride()` 查看。
2. **计算理论连续步长**：
   - 从最内层维度开始，步长是 1，然后逐层向外乘以内层维度的大小。
3. **比较实际步长和理论步长**：
   - 如果实际步长等于理论步长，张量是连续的。
   - 如果不等于，则不连续。
## 举例说明
### 例子 1：二维张量

```python
import torch
x = torch.arange(6).view(2, 3)
print(x)
# tensor([[0, 1, 2],
#         [3, 4, 5]])
print("形状:", x.shape)    # (2, 3)
print("步长:", x.stride())  # (3, 1)
```
- 形状是 `(2, 3)`。
- 理论连续步长：
  - 最内层维度（列，`d_1 = 3`）：步长 = `1`。
  - 最外层维度（行，`d_0 = 2`）：步长 = `d_1 = 3`。
  - 理论步长 = `(3, 1)`。
- 实际步长 `(3, 1)` 与理论步长 `(3, 1)` 一致，所以 `x` 是连续的。

### 例子 2：转置后的张量

```python
y = x.t()  # 转置成 3x2
print(y)
# tensor([[0, 3],
#         [1, 4],
#         [2, 5]])
print("形状:", y.shape)    # (3, 2)
print("步长:", y.stride())  # (1, 3)
```
- 形状是 `(3, 2)`。
- 理论连续步长：
  - 最内层维度（列，`d_1 = 2`）：步长 = `1`。
  - 最外层维度（行，`d_0 = 3`）：步长 = `d_1 = 2`。
  - 理论步长 = `(2, 1)`。
- 实际步长 `(1, 3)` 与理论步长 `(2, 1)` 不一致，所以 `y` 是不连续的。

### 例子 3：三维张量

```python
z = torch.arange(12).view(2, 2, 3)
print(z)
# tensor([[[ 0,  1,  2],
#          [ 3,  4,  5]],
#         [[ 6,  7,  8],
#          [ 9, 10, 11]]])
print("形状:", z.shape)    # (2, 2, 3)
print("步长:", z.stride())  # (6, 3, 1)
```
- 形状是 `(2, 2, 3)`。
- 理论连续步长：
  - 最内层（`d_2 = 3`）：步长 = `1`。
  - 中间层（`d_1 = 2`）：步长 = `d_2 = 3`。
  - 最外层（`d_0 = 2`）：步长 = `d_1 * d_2 = 2 * 3 = 6`。
  - 理论步长 = `(6, 3, 1)`。
- 实际步长 `(6, 3, 1)` 与理论步长一致，所以 `z` 是连续的。
## 为什么步长不一致就不连续？

当步长与理论值不符时，说明内存中的数据顺序已经被打乱。比如转置操作改变了逻辑上的行列顺序，但内存中的物理顺序没变，导致步长不再符合连续存储的规则。这时，访问元素需要“跳跃”到不连续的位置。
## 直接检查连续性的方法

除了手动计算步长，还可以用 PyTorch 提供的 `.is_contiguous()` 方法直接判断：

```python
print(x.is_contiguous())  # True
print(y.is_contiguous())  # False
print(z.is_contiguous())  # True
```

---
# PyTorch中Transpose操作的工作原理

## 1. 张量的内存表示

在PyTorch中，张量在内存中实际上是以一维数组的形式连续存储的。每个张量包含两部分关键信息：

- **数据指针**：指向实际存储数据的内存位置
- **元数据**：包括形状(shape)、步长(stride)等

其中，**步长(stride)** 是理解transpose操作的关键概念。步长定义了在各个维度上移动一步时，需要在内存中跳过多少个元素。

## 2. Transpose的工作原理

`transpose(dim0, dim1)`函数的作用是交换张量的两个维度，但它的实现方式非常高效：

1. **不复制数据**：transpose操作不会移动或复制内存中的实际数据
2. **只修改元数据**：它只改变张量的形状和步长信息
3. **返回视图**：transpose返回的是原始张量的一个视图(view)，共享相同的底层数据

这种设计使得transpose操作非常高效，因为它避免了数据复制的开销。

## 3. 详细示例

让我们通过一个具体例子来说明transpose的工作原理：

### 示例：2x3张量的转置

```python
import torch

# 创建一个2x3的张量
x = torch.tensor([[1, 2, 3],
                  [4, 5, 6]])
print("原始张量x:")
print(x)
print(f"形状: {x.shape}")
print(f"步长: {x.stride()}")
print(f"内存是否连续: {x.is_contiguous()}")

# 转置张量
y = x.transpose(0, 1)
print("\n转置后的张量y:")
print(y)
print(f"形状: {y.shape}")
print(f"步长: {y.stride()}")
print(f"内存是否连续: {y.is_contiguous()}")

# 查看底层存储
print("\n底层存储:")
print(x.storage().data_ptr() == y.storage().data_ptr())
```

输出结果：

```
原始张量x:
tensor([[1, 2, 3],
        [4, 5, 6]])
形状: torch.Size([2, 3])
步长: (3, 1)
内存是否连续: True

转置后的张量y:
tensor([[1, 4],
        [2, 5],
        [3, 6]])
形状: torch.Size([3, 2])
步长: (1, 3)
内存是否连续: False

底层存储:
True
```

### 解析：

1. **原始张量x**：
    
    - 形状为`[2, 3]`
    - 步长为`(3, 1)`，表示在第一维(行)移动一步需要跳过3个元素，在第二维(列)移动一步需要跳过1个元素
    - 在内存中按行优先顺序连续存储为`[1, 2, 3, 4, 5, 6]`
    
2. **转置后的张量y**：
    
    - 形状变为`[3, 2]`
    - 步长变为`(1, 3)`，表示在第一维(新的行)移动一步需要跳过1个元素，在第二维(新的列)移动一步需要跳过3个元素
    - 底层数据仍然是`[1, 2, 3, 4, 5, 6]`，但是解释方式发生了变化
    
3. **内存连续性**：
    
    - 原始张量x是连续的，因为它的元素在内存中按照行优先顺序连续存储
    - 转置后的张量y不是连续的，因为按照新的行优先顺序访问元素时，内存访问模式变成了跳跃式的

## 4. 内存布局变化的可视化

为了更好地理解transpose如何影响内存访问模式，我们可以可视化原始张量和转置后张量的内存布局：

### 原始张量（行优先，连续）:

内存: [1, 2, 3, 4, 5, 6] 访问顺序: 1→2→3→4→5→6

### 转置后的张量（不连续）:

内存: [1, 2, 3, 4, 5, 6] 访问顺序: 1→4→2→5→3→6

在转置后的张量中，按行优先顺序访问元素时，内存访问不再是连续的，这就解释了为什么转置后的张量被标记为"不连续"。

## 5. contiguous()的作用

`contiguous()`函数的作用是创建一个新的张量，将数据按照当前的逻辑布局重新排列，确保在内存中连续存储：

```python
# 创建一个连续版本的转置张量
z = y.contiguous()
print("\n连续版本的转置张量z:")
print(z)
print(f"形状: {z.shape}")
print(f"步长: {z.stride()}")
print(f"内存是否连续: {z.is_contiguous()}")
print(f"与y共享存储: {z.storage().data_ptr() == y.storage().data_ptr()}")
```

输出结果：

```
连续版本的转置张量z:
tensor([[1, 4],
        [2, 5],
        [3, 6]])
形状: torch.Size([3, 2])
步长: (2, 1)
内存是否连续: True
与y共享存储: False
```

调用`contiguous()`后：

- 创建了一个新的张量，不再与原始张量共享存储
- 数据在内存中被重新排列为`[1, 4, 2, 5, 3, 6]`
- 步长变为`(2, 1)`，符合行优先顺序
- 新张量在内存中是连续的

## 6. 在深度学习中的应用

在深度学习模型（如LLaMA）的实现中，经常需要重塑张量以适应不同的计算需求。例如：

```python
# [bsz, n_qh, s, d_h] -> [bsz, s, n_qh, d_h] -> [bsz, s, d]
output = output.transpose(1, 2).contiguous().view(bsz, seqlen, -1)
```

在这段代码中：

1. `transpose(1, 2)`交换了第二和第三维，但使张量变得不连续
2. `contiguous()`创建了一个内存连续的副本
3. `view(bsz, seqlen, -1)`将张量重塑为新的形状

`contiguous()`的调用是必要的，因为`view()`操作要求张量在内存中必须是连续的。如果尝试对非连续张量调用`view()`，PyTorch会抛出运行时错误。

## 7. 性能考虑

在优化PyTorch代码时，关于transpose和contiguous操作需要考虑以下几点：

- **transpose优势**：非常快速，因为只修改元数据而不移动数据
- **contiguous成本**：可能较高，因为需要复制和重新排列数据
- **替代方案**：有时可以使用`reshape`代替`view`+`contiguous`组合，但需确保语义相同
- **内存使用**：频繁使用`contiguous()`可能导致额外的内存消耗

在性能关键的应用中，了解这些内存布局细节并适当优化可以显著提高模型训练和推理的速度。

## 总结

PyTorch的transpose操作通过巧妙地修改张量的元数据而非实际移动数据，实现了高效的维度交换。这种设计虽然带来了性能优势，但也引入了内存不连续性的问题，在某些操作前需要使用`contiguous()`来确保正确性。理解这些底层机制对于优化深度学习模型的性能至关重要。